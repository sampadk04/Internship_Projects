{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting Up the Dependencies\n",
    "\n",
    "- Navigate to Project Directory.\n",
    "- Create a virtual environment using `virtualenv`. Activate the virtual environment afterwards.\n",
    "```bash\n",
    "virtualenv .tyre-scan-venv\n",
    "source .tyre-scan-venv/bin/activate\n",
    "```\n",
    "- Install the required dependencies listed in the `requirements.txt` file using `pip`:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- Navigate to the notebook and use the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to extract relevant urls to scrape data for given EAN Code\n",
    "\n",
    "\n",
    "# imports\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import the custom googlesearch api key\n",
    "from constants import googlesearch_api_key, googlesearch_cx, googlesearch_api_url\n",
    "# import the path to cache directory\n",
    "from constants import url_cache_dir\n",
    "# import the htmt tags to parse (or ignore)\n",
    "from constants import tags_to_use, tags_to_ignore\n",
    "# import azure openai creds\n",
    "from constants import azure_openai_creds\n",
    "# other constants\n",
    "from constants import max_chunk_size, chunk_intersection_size, llm_model\n",
    "\n",
    "# langchain imports\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain, create_extraction_chain\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "\n",
    "# utils\n",
    "from utils import remove_tags_to_ignore, extract_tags_to_use, translate_text\n",
    "\n",
    "# set azure openai variables\n",
    "os.environ[\"OPENAI_API_TYPE\"] = azure_openai_creds[\"openai_api_type\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = azure_openai_creds[\"openai_api_version\"]\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_creds[\"azure_openai_endpoint\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = azure_openai_creds[\"openai_api_key\"]\n",
    "\n",
    "# set google api variables\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = googlesearch_cx\n",
    "os.environ[\"GOOGLE_API_KEY\"] = googlesearch_api_key\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `extract_top_urls`:\n",
    "\n",
    "The function returns a JSON object containing a list of relevant queries along with additional information. Subsequently, the `extract_url_list` function is commonly employed to extract the URLs from this JSON object, providing easy access to the web resources associated with the provided EAN code. Also the actual number of results retrieved is capped at the minimum of top_k and 10 to ensure a manageable and relevant set of queries.\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. The function initializes a `GoogleSearchAPIWrapper` to interact with the Google Search API efficiently.\n",
    "\n",
    "2. It defines an inner function `top_results(query)` to retrieve the top results for a given query. This function is then utilized by the `GoogleSearchAPIWrapper` as a custom tool named \"Google Search Snippets.\"\n",
    "\n",
    "3. The main function forms a query string in the format \"EAN <ean_code>\" and uses the custom tool to fetch results from the Google Search API.\n",
    "\n",
    "4. The results are returned as a JSON object containing information about the top results, including titles, links, and snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_urls(ean_code, top_k = 5):\n",
    "    search = GoogleSearchAPIWrapper()\n",
    "\n",
    "    def top_results(query):\n",
    "        return search.results(query, min(top_k, 10))\n",
    "    \n",
    "    tool = Tool(\n",
    "        name=\"Google Search Snippets\",\n",
    "        description=\"Search Google for recent results.\",\n",
    "        func=top_results,\n",
    "    )\n",
    "\n",
    "    query = f\"EAN {ean_code}\"\n",
    "    results = tool.run(query)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `extract_url_list`:\n",
    "\n",
    "This helper function takes the results JSON object from `extract_top_urls` and extracts a list of URLs. It iterates over the result items and collects the `'link'` attribute into a list, providing a convenient way to access the URLs for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_list(results):\n",
    "    url_list = []\n",
    "    for url_dict in results:\n",
    "        url_list.append(url_dict['link'])\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scrape`:\n",
    "\n",
    "The `scrape` function is responsible for extracting HTML content from a list of URLs using the `AsyncChromiumLoader`. It opens a headless Chromium instance to load the URLs and then extracts the HTML content along with metadata. The HTML content undergoes a series of preprocessing steps, including removing unnecessary tags using `remove_tags_to_ignore`, extracting relevant tags with `extract_tags_to_use`, and optional translation using Azure Translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def scrape(urls, tags_to_use=tags_to_use, tags_to_ignore=tags_to_ignore, translation=True):\n",
    "    loader = AsyncChromiumLoader(urls)\n",
    "    docs = loader.load()\n",
    "\n",
    "    for doc in docs:\n",
    "        html_content = doc.page_content\n",
    "\n",
    "        # preprocess the html_content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # remove tags to ignore\n",
    "        soup = remove_tags_to_ignore(soup, tags_to_ignore)\n",
    "\n",
    "        # extract tags to use\n",
    "        text = extract_tags_to_use(soup, tags_to_use)\n",
    "\n",
    "        if translation:\n",
    "            text_limit = 50000\n",
    "            # translate text using azure translator\n",
    "            text = translate_text(text[:text_limit])\n",
    "        \n",
    "        # update the doc\n",
    "        doc.page_content = text\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `split_docs`\n",
    "\n",
    "The `split_docs` function divides the overall document into smaller chunks while retaining metadata. It uses `tiktoken_encoder` to split the content into chunks of a specified token size limit, with optional intersection between consecutive chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs, max_chunk_size=max_chunk_size, chunk_intersection_size=chunk_intersection_size):\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=max_chunk_size, chunk_overlap=chunk_intersection_size\n",
    "    )\n",
    "\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Validity of EAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `is_ean_valid`:\n",
    "\n",
    "The `is_ean_valid` function plays a crucial role in determining the validity of an EAN (European Article Number) code by assessing the presence of relevant Google search results associated with it. This function encapsulates the logic to identify whether an EAN code is valid or invalid based on the outcome of the `extract_top_urls` function.\n",
    "\n",
    "This function serves as a valuable tool in the EAN validation process, allowing users to quickly identify and flag invalid EAN codes based on the availability of relevant Google search results.\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. The function attempts to retrieve relevant URLs associated with the provided EAN code using the `extract_top_urls` function.\n",
    "\n",
    "2. If the `extract_top_urls` function encounters an exception during execution (such as network issues or API limitations), the function catches the exception and assigns a predefined result indicating no good Google search result was found.\n",
    "\n",
    "3. The function then compares the obtained results with the predefined invalid result. If the results match, indicating no valid URLs were found, the function concludes that the EAN code is invalid.\n",
    "\n",
    "4. The function returns a boolean value: `True` if the EAN code is valid (at least one relevant URL found) and `False` if it is invalid (no relevant URLs found).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ean_valid(ean_code):\n",
    "    invalid_ean_result = [{'Result': 'No good Google Search Result was found'}]\n",
    "    try:\n",
    "        results = extract_top_urls(ean_code)\n",
    "    except Exception as e:\n",
    "        print(e.message, e.args)\n",
    "        results = invalid_ean_result\n",
    "    \n",
    "    if results == invalid_ean_result:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ean_code = 8903094003235\n",
    "# false eg : 4250463418034\n",
    "# true eg: 8903094003235, 3528700139716\n",
    "is_ean_valid(8903094003235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract docs\n",
    "scraped_docs = scrape(extract_url_list(extract_top_urls(ean_code, top_k=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.heuver.com/product/b12400032bk0813500/12-4-32-bkt-tr-135-124a6-121a8-8pr-tt',\n",
       " 'https://shop.bohnenkamp.de/reifen-12-4-32.html',\n",
       " 'https://www.heuver.es/product/b12400032bk0813500/12-4-32-bkt-tr-135-124a6-121a8-8pr-tt',\n",
       " 'https://www.heba-reifen.at/produkt/bkt-12-4-32-tt-124a6-121a8-8pr-tr-135-as/',\n",
       " 'https://www.rengas-online.com/rshop/Renkaat/BKT/TR-135/12-4--32-8PR-TT/R-187932']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = extract_url_list(extract_top_urls(ean_code, top_k=5))\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking EAN Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `check_ean_context`:\n",
    "\n",
    "The `check_ean_context` function is a pivotal step in the process of determining whether an EAN (European Article Number) code is associated with tyres. This involves scraping and splitting HTML content from extracted URLs, storing them in a vector store, and querying Language Models (LLMs) to ascertain the association.\n",
    "\n",
    "This function serves as a valuable tool for contextual analysis, allowing users to determine the nature of the association of an EAN code with tyres through intelligent querying of Language Models.\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. The function splits the HTML content into smaller chunks using the `split_docs` function while retaining metadata.\n",
    "\n",
    "2. It initializes an Azure Chat-based OpenAI Language Model (LLM) for querying and obtaining responses.\n",
    "\n",
    "3. The Azure OpenAI Embeddings model is configured to create embeddings for the text chunks using the Azure Text Embedding deployment.\n",
    "\n",
    "4. A vector store is created using the Chroma library, incorporating the embeddings and document splits.\n",
    "\n",
    "5. The function constructs a retrieval chain to query the LLM for the association of the EAN code with tyres. The result is obtained by processing the retrieved context.\n",
    "\n",
    "6. The vector store is cleared after processing to maintain efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import custom_qa_prompt\n",
    "\n",
    "def check_ean_context(ean_code, docs, max_chunk_size=2500, chunk_intersection_size=200, k=3):\n",
    "    splits = split_docs(docs, max_chunk_size=max_chunk_size, chunk_intersection_size=chunk_intersection_size)\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=\"gpt-4\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=16\n",
    "    )\n",
    "\n",
    "    # define the embedding model\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment = \"text-embedding-ada-002\",\n",
    "        openai_api_version = azure_openai_creds[\"openai_api_version\"]\n",
    "    )\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    # retrieve and generate using the relevant snippets\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # helper function to join splits\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # define the rag chain\n",
    "    ean_context_rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    query = f\"Is this EAN code {ean_code} associated with a tyre? Return Yes or No and nothing else.\"\n",
    "\n",
    "    is_ean_context_valid = ean_context_rag_chain.invoke(query)\n",
    "\n",
    "    vectorstore.delete_collection()\n",
    "\n",
    "    if is_ean_context_valid == \"Yes\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_ean_context(ean_code, scraped_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1: RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rag_llm_pipeline`\n",
    "\n",
    "The `rag_llm_pipeline` function serves as a critical component for extracting specific information such as diameter, width, brand, model, and vehicle type associated with a validated EAN (European Article Number) code. This extraction process involves querying a Language Model (LLM) separately for each attribute, with the model retrieving relevant information based on matched context collected from a vector store.\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. The function splits the HTML content into smaller chunks using the `split_docs` function while retaining metadata.\n",
    "\n",
    "2. It initializes an Azure Chat-based OpenAI Language Model (LLM) for querying and obtaining responses.\n",
    "\n",
    "3. The Azure OpenAI Embeddings model is configured to create embeddings for the text chunks using the Azure Text Embedding deployment.\n",
    "\n",
    "4. A vector store is created using the Chroma library, incorporating the embeddings and document splits.\n",
    "\n",
    "5. The function constructs a retrieval chain to query the LLM for each specified attribute separately. The LLM responses are collected in an output dictionary.\n",
    "\n",
    "6. The vector store is cleared after processing to maintain efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import custom_qa_prompt, output_schema\n",
    "\n",
    "def rag_llm_pipeline(ean_code, docs, llm_model, max_chunk_size, chunk_intersection_size, k, output_schema):\n",
    "    splits = split_docs(docs, max_chunk_size=max_chunk_size, chunk_intersection_size=chunk_intersection_size)\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=llm_model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=16\n",
    "    )\n",
    "\n",
    "    # define the embedding model\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment = \"text-embedding-ada-002\",\n",
    "        openai_api_version = azure_openai_creds[\"openai_api_version\"]\n",
    "    )\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    # retrieve and generate using the relevant snippets\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # helper function to join splits\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # define the rag chain\n",
    "    rag_qa_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # define the output_dict\n",
    "    llm_output = {}\n",
    "\n",
    "    for key in output_schema:\n",
    "        try:\n",
    "            query = output_schema[key]\n",
    "            output = rag_qa_chain.invoke(query)\n",
    "            llm_output[key] = output\n",
    "        except Exception as e:\n",
    "            print(e.message, e.args)\n",
    "            llm_output[key] = \"NA\"\n",
    "\n",
    "    query = f\"Based on the following context extract information and generate output.\"\n",
    "\n",
    "    vectorstore.delete_collection()\n",
    "\n",
    "    return llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"gpt-35-turbo\"\n",
    "max_chunk_size = 512\n",
    "chunk_intersection_size = 128\n",
    "k = 6\n",
    "\n",
    "rag_llm_pipeline(\n",
    "    ean_code,\n",
    "    scraped_docs,\n",
    "    llm_model=llm_model,\n",
    "    max_chunk_size=max_chunk_size,\n",
    "    chunk_intersection_size=chunk_intersection_size,\n",
    "    k=k,\n",
    "    output_schema=output_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 2: Prompt Updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `prompt_llm_pipeline`:\n",
    "\n",
    "The `prompt_llm_pipeline` function is designed to iteratively extract information related to a tyre, such as diameter, width, brand, model, and vehicle type, from the scraped HTML content associated with a validated EAN (European Article Number) code. This iterative approach involves splitting the text into chunks, prompting a Language Model (LLM) for information, and updating the schema based on the model's responses.\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. The function initializes with system, user, and assistant prompts to set the context and instruct the user about the expected output format.\n",
    "\n",
    "2. The top URLs associated with the EAN code are fetched, and the corresponding HTML content is scraped and split into chunks.\n",
    "\n",
    "3. The scraped content is divided into chunks of size `context_size`, with an intersection between consecutive chunks.\n",
    "\n",
    "4. The function iterates over each chunk, prompting the LLM to extract information and update the schema.\n",
    "\n",
    "5. For the first chunk, the assistant provides an initial prompt, and for subsequent chunks, the assistant repeats the prompt while incorporating information from the previous iteration.\n",
    "\n",
    "6. The extracted information is stored and updated iteratively for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract ean data of scraped data from EAN value:\n",
    "def prompt_llm_pipeline(ean_code, context_size=13000, context_intersection=200, top_urls=None, top_k=10, translation=True):\n",
    "    '''\n",
    "    `context_size` is approximately 3.5 characters for 1 tokens.\n",
    "    We split the scraped text into chunks of sizes `context_size`, where each chunk\n",
    "    has an intersection for the last `context_intersection` characters with the previous chunk\n",
    "    and the first `context_intersection` characters with the next chunk.\n",
    "\n",
    "    We repeatedly extract and update the output of the llm model for each chunk of text, \n",
    "    before feeding the `message` as input to the llm again.\n",
    "    '''\n",
    "\n",
    "    system_prompt = \"Assistant is an intelligent chatbot designed to help users extract information on tyres based on the raw_text scraped from the web. Instructions: \\n - The extraction should include information like - diameter - width - height - price - brand - model - vehicle type of tyre.\\n - Strictly follow the output format for filling in the data. \\n - The summary should have bullet points for each of the above mentioned label. \\n Do not include information not present in the scraped data. Skip the label not available.\"\n",
    "\n",
    "    assistant_prompt_init_ = \"Here is the output format based on the above instruction. Follow this format strictly for output: \\n\\nOutput Format: \\n\\nAccording to the scraped data, the following information is available:\\n\\n- Diameter: [insert diameter here]\\n- Width: [insert width here]\\n- Height: [insert height here]\\n- Price: [insert price or price range here]\\n- Brand: [insert brand here]\\n- Model: [insert model here]\\n- Vehicle Type of Tyre: [insert vehicle type here]\\n\\n\"\n",
    "\n",
    "    assistant_prompt_repeat_ = \"Here is the output obtained from the previous chunk of scraped data. Update this output following these instructions:\\n -Here are the outputs from previous iteration, so make sure to retain all the non-redundant information.\\n - Change value of any key having \\\"Not available\\\", only after getting evidence from current chunk.\\n - Follow this output format strictly for output.\\n - Update value to multiple values or range depending on updated information from the chunk.\\n\\n So, here is the output from previous iteration:\\n\\n\"\n",
    "    \n",
    "    if not(top_urls):\n",
    "        # extract top_k urls\n",
    "        top_urls = get_top_urls(ean_code, top_k)\n",
    "\n",
    "    # scrape urls and split the text\n",
    "    scraped_content = final_scraper(top_urls, translation=translation)\n",
    "\n",
    "    # initialize an empty list to store the chunks\n",
    "    chunks = []\n",
    "\n",
    "    # split the scraped content into chunks of size `context_size`\n",
    "\n",
    "    # loop over the scraped content\n",
    "    for i in range(0, len(scraped_content), 13800):\n",
    "        # append the chunk to the list\n",
    "        chunks.append(scraped_content[i:i+13000])\n",
    "    \n",
    "    \n",
    "    # handle each chunk separately\n",
    "    \n",
    "    # for the initial prompt use system_prompt + chunk_prompt_init + assistant_prompt_init\n",
    "    # for the remaining loop prompts use system_prompt + chunk_prompt_repeat + assistant_prompt_repeat\n",
    "        \n",
    "    # initialize a string to hold the llm output from previous iteration\n",
    "    llm_output = \"\"\n",
    "        \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # if first chunk\n",
    "        if i == 0:\n",
    "            chunk_prompt_init = \"\\n\\n Web Scraped Chunk No. \" + str(i+1) + \":\\n\\n\"\n",
    "            chunk_prompt_init += \"\\n\\n\" + chunk + \"\\n\\n\"\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk_prompt_init},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_prompt_init_}\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "\n",
    "            chunk_prompt_repeat = \"\\n\\n Web Scraped Chunk No. \" + str(i+1) + \":\\n\\n\"\n",
    "            chunk_prompt_repeat += \"\\n\\n\" + chunk + \"\\n\\n\"\n",
    "\n",
    "            assistant_prompt_repeat_ += \"\\n\\n\" + llm_output + \"\\n\\n\" \n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk_prompt_repeat},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_prompt_repeat_},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_prompt_init_}\n",
    "            ]\n",
    "\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=engine,\n",
    "            temperature=0.0, #make sure the outputs are deterministic\n",
    "            messages=messages,\n",
    "            max_tokens= 200\n",
    "        )\n",
    "\n",
    "        # store the extracted data\n",
    "        llm_output = response['choices'][0]['message']['content']\n",
    "\n",
    "    return llm_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tyre-scan-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
