{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# Mediapipe Blazepose\n",
    "import mediapipe as mp\n",
    "\n",
    "# Path Management\n",
    "import os\n",
    "\n",
    "# General Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapose Utilities\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Creating Visualizations with Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "def create_visualization(video_path):\n",
    "    ## these are 16 tuples of keypoints to extract 18 relevant angles\n",
    "    keypoint_index = np.array([\n",
    "        [33, 16, 14],\n",
    "        [16, 14, 12],\n",
    "        [14, 12, 11],\n",
    "        [14, 12, 24],\n",
    "        [ 0, 34, 12],\n",
    "        [ 0, 34, 36],\n",
    "        [12, 11, 13],\n",
    "        [13, 11, 23],\n",
    "        [11, 13, 15],\n",
    "        [13, 15, 35],\n",
    "        [12, 24, 26],\n",
    "        [26, 24, 23],\n",
    "        [24, 23, 25],\n",
    "        [11, 23, 25],\n",
    "        [24, 26, 28],\n",
    "        [23, 25, 27],\n",
    "        [26, 28, 32],\n",
    "        [25, 27, 31]\n",
    "    ])\n",
    "\n",
    "    # no. of angles to consider: k\n",
    "    k = keypoint_index.shape[0]\n",
    "\n",
    "    # we make a boolean array to store the angles we would like to visualize\n",
    "    angles_to_visualize = np.zeros(k)\n",
    "\n",
    "    # set the angles to be visualized to 1 (True)\n",
    "    angles_to_visualize[1] = 1\n",
    "    angles_to_visualize[3] = 1\n",
    "    angles_to_visualize[4] = 1\n",
    "    angles_to_visualize[7] = 1\n",
    "    angles_to_visualize[8] = 1\n",
    "    angles_to_visualize[10] = 1\n",
    "    angles_to_visualize[12] = 1\n",
    "    angles_to_visualize[14] = 1\n",
    "    angles_to_visualize[15] = 1\n",
    "\n",
    "    # Establishing connection with the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Properties\n",
    "    # extracting video properties\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    size = (width, height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('a','v','c','1')\n",
    "    op_path = os.path.join('data', 'output_videos', 'output.avi')\n",
    "    # Video Writer \n",
    "    video_writer = cv2.VideoWriter(op_path, fourcc, fps, size)\n",
    "\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "\n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # we initialize to size (37,3) instead of (33,3) because we include 4 more key-points, for angle calculation\n",
    "            frame_coordinates = np.zeros((37,3))\n",
    "\n",
    "            ## extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "            \n",
    "            ## creating 4 new landmarks\n",
    "            # left hand key-point\n",
    "            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n",
    "            # right hand key-point\n",
    "            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n",
    "            # neck key-point\n",
    "            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n",
    "            # middle pelvis key-point\n",
    "            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n",
    "\n",
    "            # initialize array to hold angles per frame\n",
    "            angle = np.zeros(k)\n",
    "\n",
    "            ## extract angles based on the relevant key-points from the keypoint_index\n",
    "            for i in range(k):\n",
    "                # calculate the angle based on the key-point indices as in the tuple from keypoint_index\n",
    "                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n",
    "\n",
    "            # Visualize angles\n",
    "            for i in range(k):\n",
    "                if angles_to_visualize[i]:\n",
    "                    theta = int(angle[i] * (180/np.pi))\n",
    "                    landmark_index = keypoint_index[i][1]\n",
    "                    # (x,y) coordinates of landmark\n",
    "                    landmark_coordinates = frame_coordinates[landmark_index][:2]\n",
    "\n",
    "                    cv2.putText(image,\n",
    "                                str(theta),\n",
    "                                tuple(np.multiply(landmark_coordinates, list(size)).astype(int)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # write the frame\n",
    "            video_writer.write(image)\n",
    "\n",
    "\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "        # Releasing video writer\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we extract the coordinates of 33 keypoints using Mediapipe Blazepose. These are the keypoints and their corresponding labels:\n",
    "    - ![](https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png)\n",
    "\n",
    "- We add 4 more key-points namely:\n",
    "    - `33`: left-hand\n",
    "    - `34`: neck\n",
    "    - `35`: right-hand\n",
    "    - `36`: middle-pelvis\n",
    "\n",
    "- We employ cube-normalization by applying the following transformation to each coordinate in each frame:\n",
    "    - $(x,y,z) \\mapsto (\\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}, \\frac{y - y_{\\text{min}}}{y_{\\text{max}} - y_{\\text{min}}}, \\frac{z - z_{\\text{min}}}{z_{\\text{max}} - z_{\\text{min}}})$\n",
    "    - This makes $(x_{\\text{min}}, y_{\\text{min}}, z_{\\text{min}})$ the origin of the unit cube coordinate system.\n",
    "\n",
    "- We then consider 18-relevant joint angles, using the key-point coordinates:\n",
    "    - ![](https://i.postimg.cc/zBSdvndp/image.png)\n",
    "\n",
    "- We extract these 18 angles for each frame, and compile these into a `(n, 18)` shaped array, where `n` is the no. of frames in the video.\n",
    "\n",
    "- Then we use DTW to compare these arrays of the videos to compare and obtain a score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function: calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    \n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "## helper function: to extract (n, k) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## these are k tuples of keypoints to extract k relevant angles\n",
    "## k = keypoint_index.shape[0]\n",
    "keypoint_index = np.array([\n",
    "    [33, 16, 14],\n",
    "    [16, 14, 12],\n",
    "    [14, 12, 11],\n",
    "    [14, 12, 24],\n",
    "    [ 0, 34, 12],\n",
    "    [ 0, 34, 36],\n",
    "    [12, 11, 13],\n",
    "    [13, 11, 23],\n",
    "    [11, 13, 15],\n",
    "    [13, 15, 35],\n",
    "    [12, 24, 26],\n",
    "    [26, 24, 23],\n",
    "    [24, 23, 25],\n",
    "    [11, 23, 25],\n",
    "    [24, 26, 28],\n",
    "    [23, 25, 27],\n",
    "    [26, 28, 32],\n",
    "    [25, 27, 31]\n",
    "])\n",
    "\n",
    "def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n",
    "    # no. of angles to consider: k\n",
    "    k = keypoint_index.shape[0]\n",
    "    # the list to store the 'k' angles per frame\n",
    "    angles = []\n",
    "    \n",
    "    # extablishing video capture (not necessary if you have extracted the coordinates already in some array, then directly use those info: this is preferable when we are using a limited no. of videos for testing purposes)\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # setting up mediapipe blazepose\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from video\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # we initialize to size (37,3) instead of (33,3) because we include 4 more key-points, for angle calculation\n",
    "            frame_coordinates = np.zeros((37,3))\n",
    "\n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            ## extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "            \n",
    "            ## creating 4 new landmarks\n",
    "            # left hand key-point\n",
    "            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n",
    "            # right hand key-point\n",
    "            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n",
    "            # neck key-point\n",
    "            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n",
    "            # middle pelvis key-point\n",
    "            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n",
    "\n",
    "            ## normalize frame_coordinates into a unit cube with (x_min, y_min, z_min) as the origin\n",
    "            for i in range(37):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "\n",
    "            # initialize array to hold angles per frame\n",
    "            angle = np.zeros(k)\n",
    "\n",
    "            ## extract angles based on the relevant key-points from the keypoint_index\n",
    "            for i in range(k):\n",
    "                # calculate the angle based on the key-point indices as in the tuple from keypoint_index\n",
    "                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n",
    "            \n",
    "            angles.append(angle)\n",
    "\n",
    "    # Releasing the video capture\n",
    "    cap.release()\n",
    "\n",
    "    # convert the list into an np array of size: (n, k)\n",
    "    return np.array(angles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (n, k) using DTW\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    ## normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "\n",
    "    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n",
    "    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n",
    "    \n",
    "    # calculate distance\n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    # we give a score out of 100\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score\n",
    "\n",
    "\n",
    "\n",
    "## comparing videos by path: just including the pre-processing steps in the previous function\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_angle_arr(vid1_path)\n",
    "    vid2_arr  = extract_angle_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n",
    "\n",
    "    print(\"Score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to compare right videos with every other right video and every wrong video to right video and give average right and average wrong scores\n",
    "\n",
    "def compare_right_wrong_vid(right_paths, wrong_paths):\n",
    "    # comparing right videos with each other\n",
    "    r_score = 0\n",
    "    r_count = 0\n",
    "\n",
    "    for i in range(len(right_paths)):\n",
    "        for j in range(i+1, len(right_paths)):\n",
    "            r_score += combined_compare(right_paths[i], right_paths[j])\n",
    "            r_count += 1\n",
    "    \n",
    "\n",
    "    # comparing wrong videos with right videos\n",
    "    w_score = 0\n",
    "    w_count = 0\n",
    "\n",
    "    for i in range(len(right_paths)):\n",
    "        for j in range(len(wrong_paths)):\n",
    "            w_score += combined_compare(right_paths[i], wrong_paths[j])\n",
    "            w_count += 1\n",
    "    \n",
    "\n",
    "    # print out the average scores\n",
    "    print(\"Average Right Video Comparison Score:\", r_score/r_count)\n",
    "    print(\"Average Wrong Video Comparison Score:\", w_score/w_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nr1_path = os.path.join('data', 'exercise_name', 'R1.1.mp4')\\nr2_path = os.path.join('data', 'exercise_name', 'R1.2.mp4')\\nw1_path = os.path.join('data', 'exercise_name', 'W2.1.mp4')\\nw2_path = os.path.join('data', 'exercise_name', 'W2.2.mp4')\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract video paths\n",
    "\n",
    "# this is the format to follow to read the video paths\n",
    "'''\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'exercise_name', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'exercise_name', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'exercise_name', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'exercise_name', 'W2.2.mp4')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cable Bar Curls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'cableBarCurls', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'cableBarCurls', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'cableBarCurls', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'cableBarCurls', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 89.14991920043461\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 46.979874979781314\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 61.09276857550815\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 53.06287335139201\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 66.85947754075687\n",
      "Average Right Video Comparison Score: 89.14991920043461\n",
      "Average Wrong Video Comparison Score: 56.99874861185958\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deadlift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'deadlift', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'deadlift', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'deadlift', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'deadlift', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 92.12561189490877\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 67.6504185386642\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 71.13574131573282\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 68.44258993337988\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 71.45158376885081\n",
      "Average Right Video Comparison Score: 92.12561189490877\n",
      "Average Wrong Video Comparison Score: 69.67008338915693\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Straight Leg Kickbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'straightLegKickBacks', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'straightLegKickBacks', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'straightLegKickBacks', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'straightLegKickBacks', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 91.4533835262837\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 69.75813874067939\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 68.18027765552351\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.93731415901529\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 75.54205155919934\n",
      "Average Right Video Comparison Score: 91.4533835262837\n",
      "Average Wrong Video Comparison Score: 72.60444552860437\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reverse Fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'reverseFly', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'reverseFly', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'reverseFly', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'reverseFly', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 94.37287377117931\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 85.5332480964526\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.00508756289773\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 87.71333930212397\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 84.18538364044745\n",
      "Average Right Video Comparison Score: 94.37287377117931\n",
      "Average Wrong Video Comparison Score: 84.60926465048044\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'plank', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'plank', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'plank', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'plank', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 89.07789432536592\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.6742562197015\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 69.19085114933112\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 84.74352407546873\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 68.35083031146648\n",
      "Average Right Video Comparison Score: 89.07789432536592\n",
      "Average Wrong Video Comparison Score: 76.23986543899196\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lunges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'lunges', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'lunges', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'lunges', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'lunges', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 85.67162065829733\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.82896995031554\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.14782193063787\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.30588164671367\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.22446589623385\n",
      "Average Right Video Comparison Score: 85.67162065829733\n",
      "Average Wrong Video Comparison Score: 79.37678485597525\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Front Arm Raises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'frontArmRaise', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'frontArmRaise', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'frontArmRaise', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'frontArmRaise', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 93.05305289295296\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 66.71510872328682\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 71.7991619894982\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 73.49533747370944\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 78.40156645002233\n",
      "Average Right Video Comparison Score: 93.05305289295296\n",
      "Average Wrong Video Comparison Score: 72.60279365912919\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Donkey Kick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'donkeyKick', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'donkeyKick', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'donkeyKick', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'donkeyKick', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 85.30634907282868\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.29168594998531\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 85.98184515014701\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 84.88709312258841\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 85.98423018019112\n",
      "Average Right Video Comparison Score: 85.30634907282868\n",
      "Average Wrong Video Comparison Score: 82.28621360072796\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bulgarian Split Squats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'bulgarianSplitSquats', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'bulgarianSplitSquats', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'bulgarianSplitSquats', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'bulgarianSplitSquats', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 91.87764475790397\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 84.3347686018169\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 85.43503430466023\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.83641783216899\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.61782778215877\n",
      "Average Right Video Comparison Score: 91.87764475790397\n",
      "Average Wrong Video Comparison Score: 83.55601213020122\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Boat Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'boatPose', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'boatPose', 'R1.2.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'boatPose', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'boatPose', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 94.92409455573463\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.69083770589035\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.37257321250205\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.97555271615909\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.5909439770787\n",
      "Average Right Video Comparison Score: 94.92409455573463\n",
      "Average Wrong Video Comparison Score: 80.65747690290755\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Dumbbell Curls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract video paths\n",
    "\n",
    "r1_path = os.path.join('data', 'non-vanilla_test_data', 'dumbbellCurls', 'R1.1.mp4')\n",
    "r2_path = os.path.join('data', 'non-vanilla_test_data', 'dumbbellCurls', 'R1.2.mp4')\n",
    "r3_path = os.path.join('data', 'non-vanilla_test_data', 'dumbbellCurls', 'R1.3.mp4')\n",
    "w1_path = os.path.join('data', 'non-vanilla_test_data', 'dumbbellCurls', 'W2.1.mp4')\n",
    "w2_path = os.path.join('data', 'non-vanilla_test_data', 'dumbbellCurls', 'W2.2.mp4')\n",
    "\n",
    "r_paths = [r1_path, r2_path, r3_path]\n",
    "w_paths = [w1_path, w2_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 86.9791943786166\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.48480494259894\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.87120014194673\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.57485928147821\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.04266675764134\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.58970989682184\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.03272737020221\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 83.4072183973026\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.91644868978454\n",
      "Average Right Video Comparison Score: 83.7783998210541\n",
      "Average Wrong Video Comparison Score: 78.26060506553846\n"
     ]
    }
   ],
   "source": [
    "compare_right_wrong_vid(r_paths, w_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "# extract a visualization\n",
    "create_visualization(r1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ed39f92146e9c2db1a29729bb43dc2eb9581f5c121f265c48b6aafb302e3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
