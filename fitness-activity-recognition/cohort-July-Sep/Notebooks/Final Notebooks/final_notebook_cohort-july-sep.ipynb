{"cells":[{"cell_type":"markdown","metadata":{"id":"hmkFJT6tqrHo"},"source":["# Final Notebook - Fitness Activity Recognition - Cohort 22"]},{"cell_type":"markdown","metadata":{"id":"4fV7pxdeqrHt"},"source":["# 0. Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fr1SjXn3qrHt"},"outputs":[],"source":["# Disable Warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olD3CE10qrHu"},"outputs":[],"source":["# OpenCV to read and write the videos\n","import cv2\n","\n","# For Mediapipe Blazepose to extract key-point coordinates\n","import mediapipe as mp\n","\n","# Standard libraries\n","import numpy as np\n","import pandas as pd\n","\n","# For file-handling\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-wEq0PUqrHv"},"outputs":[],"source":["# Mediapipe Blazepose utilities\n","\n","mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","mp_pose = mp.solutions.pose"]},{"cell_type":"markdown","metadata":{"id":"tC6QUgqiqrHv"},"source":["# 1.1 Dataset 0\n","\n","We use 3 different videos of subjects doing bicep curls for preliminary comparison:\n","- Video 1: Trainer Video\n","- Video 2: Trainee Video with correct form\n","- Video 3: Trainee Video with wrong form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKdMfOUWqrHv","outputId":"2d0fcbee-09c9-478b-e47c-6ba46e5a086e"},"outputs":[{"name":"stdout","output_type":"stream","text":["data/sample_videos/curls3-1.mp4\n","data/sample_videos/curls3-2.mp4\n","data/sample_videos/curls3-3.mp4\n"]}],"source":["# storing the video file paths\n","\n","vid1_path = os.path.join('data', 'sample_videos', 'curls3-1.mp4')\n","print(vid1_path)\n","\n","vid2_path = os.path.join('data', 'sample_videos', 'curls3-2.mp4')\n","print(vid2_path)\n","\n","vid3_path = os.path.join('data', 'sample_videos', 'curls3-3.mp4')\n","print(vid3_path)"]},{"cell_type":"markdown","metadata":{"id":"hQ3E0AqKqrHw"},"source":["# 1.2 Dataset 1\n","\n","We use 3 different videos taken from 3 different angles of subjects doing bicep curls for testing of angle invariance algorithms:\n","- Video 1: Front View Video\n","- Video 2: Left View Video\n","- Video 3: Right View Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPWlOfYaqrHw","outputId":"e8c10bc3-7b31-4dfb-bc98-e1a4a38e7662"},"outputs":[{"name":"stdout","output_type":"stream","text":["data/sample_videos/curls_R-F-1.mp4\n","data/sample_videos/curls_R-L-1.mp4\n","data/sample_videos/curls_R-R-1.mp4\n"]}],"source":["# storing the video file paths\n","\n","vidf_path = os.path.join('data', 'sample_videos', 'curls_R-F-1.mp4')\n","print(vidf_path)\n","\n","vidl_path = os.path.join('data', 'sample_videos', 'curls_R-L-1.mp4')\n","print(vidl_path)\n","\n","vidr_path = os.path.join('data', 'sample_videos', 'curls_R-R-1.mp4')\n","print(vidr_path)"]},{"cell_type":"markdown","metadata":{"id":"ya1ViNTMqrHx"},"source":["# 2. Preliminary Algorithms"]},{"cell_type":"markdown","metadata":{"id":"xDZxL28tqrHx"},"source":["## Algorithm 1.1\n","\n","We first reshape the `(n, 33, 3)` dimensional array to `(33, n, 3)` dimensional array (here `n` is the no. of frames in the input video). Then we loop over each of the `33` key-points individually. Let’s say we are focussing on key-point `0`, which corresponds to the nose. We consider the `(n1, 3)` and `(n2, 3)` dimensional arrays corresponding to this key-point in the first and second video respectively and use DTW to obtain a score corresponding to each key-point. We obtain 33 such scores and consider the average score out of all."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xhJNwp7qrHx"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","            \n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","\n","            # extract (x,y,z) coordinate of each landmark (key-point)\n","            for i in range(33):\n","                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","## reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates\n","        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n","        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"Ab9zvDL5qrHy"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIhnRe-XqrHy","outputId":"5494dbe1-732f-49a3-8933-cb8908a4602d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n"," 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n"," 100. 100. 100. 100. 100.]\n","Average Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC2fdalWqrHz","outputId":"4f2f46c3-0063-4ff4-a232-62a891fc57f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [90.36712509 90.48350076 90.4214465  90.35077107 89.98707817 89.96469313\n"," 89.93794297 90.28094662 88.2643645  90.54121127 89.79774844 91.68475828\n"," 87.10831677 92.99999853 87.92232255 88.48792062 81.77806401 88.23772274\n"," 80.51475224 88.83917342 81.23785002 87.93834118 81.52105547 92.34317024\n"," 91.37831678 91.96892653 92.49469029 92.16970594 92.91024929 92.23795573\n"," 92.89406741 92.15368687 92.04274271]\n","Average Score: 89.4321398829224\n"]}],"source":["combined_compare(vid1_path, vid2_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axoTIHIEqrHz","outputId":"0aa509c0-f9fa-4e0d-a62b-de6216e69561"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [77.64975877 77.51754387 77.55602626 77.60267612 77.69823648 77.72383293\n"," 77.76272665 77.87427589 78.52889323 77.89025921 78.09730396 79.23676164\n"," 79.29086474 84.07772858 80.88352592 76.7209857  69.06288637 75.14808023\n"," 67.02120509 74.80446291 66.30122747 74.34038988 67.36503401 80.99545498\n"," 81.17073534 79.05177224 77.54065719 77.68160555 76.73332758 77.80852284\n"," 76.83242531 76.98610586 76.16014382]\n","Average Score: 76.7004677761122\n"]}],"source":["combined_compare(vid1_path, vid3_path)"]},{"cell_type":"markdown","metadata":{"id":"fbYkweoyqrHz"},"source":["## Algorithm 1.2\n","\n","This is a modification of **Algorithm 1.1**. We use our domain expertise about the exercise, i.e. bicep curls and argue that only the key-points corresponding to the upper body matters. Hence we limit our observations to only the key-points in the upper body and only consider `(n, k , 3)` dimensional array, where `k` is the no. of key-points considered `(k  33)`. Everything else is exactly the same as before, and we end up considering the average score of these `k` key-points."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BY5HNMZqqrHz"},"outputs":[],"source":["## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the coordinates of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n","\n","def extract_arr(input_video_path, landmark_list = list(range(33))):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # we sort the landmark_list inplace for convenience\n","            landmark_list.sort()\n","\n","            # initialize frame_coordinates to store the coordinates of the 'len(landmark_list)' key-points per frame\n","            frame_coordinates = np.zeros((len(landmark_list),3))\n","\n","            # extract (x,y,z) coordinate of each landmark (key-point)\n","            for i in range(len(landmark_list)):\n","                idx = landmark_list[i]\n","                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates), landmark_list\n","\n","\n","## reshape the (n, len(landmark_list), 3) dimensional array to a (len(landmark_list), n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, len(landmark_list), 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates\n","        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n","        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n","    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n","    vid2_arr, _  = extract_arr(vid2_path, landmark_list)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"eY2vgFaQqrH0"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N43E_v1GqrH0"},"outputs":[],"source":["# we consider landmarks from 0 till 24: to consider only the upper body\n","landmark_list = list(range(25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yioLp8CMqrH1","outputId":"107ba614-1e48-48a2-c09b-e047cd1054e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n"," 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n","Average Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path, landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hPACP-eqrH1","outputId":"fe1efaaf-c596-43fe-c6a9-1ce195e9046d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [90.36712509 90.48350076 90.4214465  90.35077107 89.98707817 89.96469313\n"," 89.93794297 90.28094662 88.2643645  90.54121127 89.79774844 91.68475828\n"," 87.10831677 92.99999853 87.92232255 88.48792062 81.77806401 88.23772274\n"," 80.51475224 88.83917342 81.23785002 87.93834118 81.52105547 92.34317024\n"," 91.37831678]\n","Average Score: 88.4955436546632\n"]}],"source":["combined_compare(vid1_path, vid2_path, landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0FUBl5sqrH1","outputId":"c55d911b-da35-449c-a9a8-38cbfdd144e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [77.64975877 77.51754387 77.55602626 77.60267612 77.69823648 77.72383293\n"," 77.76272665 77.87427589 78.52889323 77.89025921 78.09730396 79.23676164\n"," 79.29086474 84.07772858 80.88352592 76.7209857  69.06288637 75.14808023\n"," 67.02120509 74.80446291 66.30122747 74.34038988 67.36503401 80.99545498\n"," 81.17073534]\n","Average Score: 76.49283504900352\n"]}],"source":["combined_compare(vid1_path, vid3_path, landmark_list)"]},{"cell_type":"markdown","metadata":{"id":"Gwo_xPO1qrH1"},"source":["## Algorithm 2.1\n","\n","Instead of reshaping the array as in **Algorithm 1.1**, **1.2**, we now consider the whole `(n, (33,3))` array of each video for comparison. So, each frame contains information of all the key-points in a `(33, 3)` dimensional array. We use DTW on this and consider the score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jTTLY6CqrH1"},"outputs":[],"source":["## to create a helper function to extract (n, (33, 3)) dimensional array to store the coordinates of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","            \n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","\n","            # extract (x,y,z) coordinate of each landmark (key-point)\n","            for i in range(33):\n","                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, (33, 3)) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    # normalize the coordinates\n","    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n","    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n","    \n","    d = distance(vid1_arr, vid2_arr)\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    score = compare_vid(vid1_arr, vid2_arr)\n","\n","    print(\"Final Score:\", score)"]},{"cell_type":"markdown","metadata":{"id":"eWgomLSTqrH2"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnfrDHIcqrH2","outputId":"560aa864-9df2-4e37-b982-7ca075e99f5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyQ65rKVqrH2","outputId":"4603f006-c8a0-427c-c9e7-09909ee48c56"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 85.61251023753023\n"]}],"source":["combined_compare(vid1_path, vid2_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0JVrAYxqrH2","outputId":"5d0d5cf3-9508-4081-a8df-fa250c89c672"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 72.13579475938042\n"]}],"source":["combined_compare(vid1_path, vid3_path)"]},{"cell_type":"markdown","metadata":{"id":"dJwxyEsUqrH3"},"source":["## Algorithm 2.2\n","\n","This is again a modification of **Algorithm 2.1** and **Algorithm 1.2**, where we limit ourselves to the relevant key-points (in this case the upper-body) and directly compare the `(n, (k, 3))` dimensional arrays of each of the videos to obtain the score. So, each frame contains information of only the relevant `k` key-points (of the upper-body) in the `(k, 33)` dimensional array. We use DTW on this to consider the score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGG71g2sqrH3"},"outputs":[],"source":["## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the coordinates of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n","\n","def extract_arr(input_video_path, landmark_list = list(range(33))):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # we sort the landmark_list inplace for convenience\n","            landmark_list.sort()\n","\n","            # initialize frame_coordinates to store the coordinates of the 'len(landmark_list)' key-points per frame\n","            frame_coordinates = np.zeros((len(landmark_list),3))\n","            \n","            # extract (x,y,z) coordinate of each landmark (key-point)\n","            for i in range(len(landmark_list)):\n","                idx = landmark_list[i]\n","                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates), landmark_list\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, len(landmark_list), 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    \n","    # normalize the coordinates\n","    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n","    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n","    \n","    d = distance(vid1_arr, vid2_arr)\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n","    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n","    vid2_arr, _ = extract_arr(vid2_path, landmark_list)\n","\n","    score = compare_vid(vid1_arr, vid2_arr)\n","\n","    print(\"Final Score:\", score)"]},{"cell_type":"markdown","metadata":{"id":"2Rqj9f6DqrH3"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Wg1-iR0qrH4"},"outputs":[],"source":["# we consider landmarks from 0 till 24: to consider only the upper body\n","landmark_list = list(range(25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1S60ZRE1qrH4","outputId":"268ed2e0-b464-4946-842a-94455e120425"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path,landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2piXxmgqrH5","outputId":"93b0f643-5815-40cd-ccce-6ad3e3c144d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 83.77952859907592\n"]}],"source":["combined_compare(vid1_path, vid2_path,landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHGlbyGqqrH5","outputId":"16216ede-fb51-4ae5-e9e9-e2acb5098f8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 70.6449503249911\n"]}],"source":["combined_compare(vid1_path, vid3_path,landmark_list)"]},{"cell_type":"markdown","metadata":{"id":"UtkzQEJFqrH5"},"source":["# 3. Algorithms with Cube Normalization\n","\n","As in the previous Algorithms, we again use Mediapipe blazepose to extract 33-key points coordinates in the `x`,`y` and `z` axis, which is normalized w.r.t. the size of the frame. But this time we go one step further and implement the “Cube Normalization”.\n","\n","**Cube Normalization:**\n","For each frame, first we extract the $(x_{\\text{min}}, y_{\\text{min}}, z_{\\text{min}})$ and $(x_{\\text{max}}, y_{\\text{max}}, z_{\\text{max}})$ which are the min and max coordinates of each of the components, amongst all the 33 key-point coordinates in a particular frame.\n","We use these min-max coordinates to transform the coordinates of the 33 key-points using the following equation:\n","$$(x,y,z) \\mapsto  (\\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}, \\frac{y - y_{\\text{min}}}{y_{\\text{max}} - y_{\\text{min}}}, \\frac{z - z_{\\text{min}}}{z_{\\text{max}} - z_{\\text{min}}})$$\n","\n","After this transformation, we only focus on the key-points of the subject, which makes the background obsolete. This also manages to squeeze all the coordinates into a unit cube, with the origin translated to the $(x_{\\text{min}}, y_{\\text{min}}, z_{\\text{min}})$."]},{"cell_type":"markdown","metadata":{"id":"d_5ETqImqrH6"},"source":["## Algorithm 3.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaFNoJOcqrH6"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z\n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","## reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates\n","        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n","        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"aspBCzIQqrH6"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kzf0AAcjqrH7","outputId":"d54f5245-414d-422a-9c9a-ee844d7bda2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n"," 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n"," 100. 100. 100. 100. 100.]\n","Average Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1210ZB1YqrH7","outputId":"3fe9e9ce-ecf1-4e0c-92ff-f18b367850c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [76.09880093 71.03308007 71.92827159 72.97813549 71.17779816 71.32982945\n"," 71.62863353 83.42066047 80.79698083 83.79111416 83.92837262 91.99387374\n"," 89.42282927 89.75248382 84.20653405 80.34372698 74.62355477 77.20345952\n"," 71.53413893 77.28875337 72.1713712  78.79991038 74.87175014 87.50617816\n"," 86.33696332 82.74433482 88.15196298 92.0240622  93.22718093 93.21799267\n"," 93.55924829 90.54064646 93.09000439]\n","Average Score: 81.84007992994708\n"]}],"source":["combined_compare(vid1_path, vid2_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8FJLNcYqrH7","outputId":"9bdf8c4a-b516-441a-8104-4b3fe8f4e22d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [70.64647839 72.04212754 72.4391972  72.87225625 70.89557378 71.13176141\n"," 71.47964269 76.64926065 73.51485129 73.78181789 72.46286718 81.5419961\n"," 79.96752798 66.27161632 75.31776237 59.18279573 65.48069597 52.85263092\n"," 59.70680572 52.3766379  57.8763038  55.54283735 61.07838155 74.01273037\n"," 75.66878705 73.65711004 77.58279603 79.34605448 79.69506302 80.02573554\n"," 80.03066776 78.59057671 80.22067364]\n","Average Score: 71.02854607963663\n"]}],"source":["combined_compare(vid1_path, vid3_path)"]},{"cell_type":"markdown","metadata":{"id":"HdRf1nDvqrH8"},"source":["## Algorithm 3.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNxWNo3sqrH8"},"outputs":[],"source":["## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n","\n","def extract_arr(input_video_path, landmark_list = list(range(33))):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # we sort the landmark_list inplace for convenience\n","            landmark_list.sort()\n","            \n","            # initialize to store the key-points coordinates in the frame\n","            frame_coordinates = np.zeros((len(landmark_list),3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(len(landmark_list)):\n","                idx = landmark_list[i]\n","                x, y, z = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(len(landmark_list)):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates), landmark_list\n","\n","\n","## reshape the (n, len(landmark_list), 3) dimensional array to a (len(landmark_list), n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, len(landmark_list), 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates\n","        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n","        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n","    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n","    vid2_arr, _  = extract_arr(vid2_path, landmark_list)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"SlIobwaHqrH9"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-G3ZjyRqrH9"},"outputs":[],"source":["# we consider landmarks from 0 till 24: to consider only the upper body\n","landmark_list = list(range(25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VI55kcOJqrH9","outputId":"f1dac832-b9e2-4e42-b68e-96b80818bef0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n"," 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n","Average Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path,landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vstnSnvaqrH9","outputId":"94f498e8-1b56-4f70-d191-e8151d850128"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [79.29076533 74.62334593 75.25919102 76.04498242 76.75394097 76.75762224\n"," 76.875806   85.18942254 85.8666015  85.01640652 86.24877752 93.15801846\n"," 88.64307079 87.08987375 83.57118547 88.67744092 74.7101584  87.44175462\n"," 73.97601561 87.14164671 75.5954874  87.54069156 76.52657353 84.99043884\n"," 85.98794896]\n","Average Score: 82.11908668037474\n"]}],"source":["combined_compare(vid1_path, vid2_path,landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjTxnZaLqrH9","outputId":"c0eb44a8-ad54-434b-849e-98927bcf383a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [77.35746198 77.4755386  77.96513494 78.49942307 77.01953939 77.18586414\n"," 77.48336462 81.13451253 79.70358058 79.9590351  79.50166629 81.43148999\n"," 85.53170833 65.59291866 76.88160367 52.32549177 69.21050326 49.27063941\n"," 66.29886869 49.27494289 64.55502344 50.97623    66.85073321 75.39640356\n"," 76.66159941]\n","Average Score: 71.7417311011076\n"]}],"source":["combined_compare(vid1_path, vid3_path,landmark_list)"]},{"cell_type":"markdown","metadata":{"id":"JzV9yH-kqrH9"},"source":["## Algorithm 4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSMcCmLgqrH-"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z\n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    # normalize the coordinates\n","    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n","    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n","    \n","    d = distance(vid1_arr, vid2_arr)\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    score = compare_vid(vid1_arr, vid2_arr)\n","\n","    print(\"Final Score:\", score)"]},{"cell_type":"markdown","metadata":{"id":"1tSgLLXSqrH-"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9CprFphqrH-","outputId":"3375af0c-9ba5-49de-b2e8-80f6aa353c72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yL3fh1jVqrH-","outputId":"799ebfa1-38b8-4aa4-98d9-eec26685963b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 80.65988501532588\n"]}],"source":["combined_compare(vid1_path, vid2_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y2Rj73DqrH-","outputId":"581142b6-1100-4ca5-84ba-e2890598501a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 67.32642349397406\n"]}],"source":["combined_compare(vid1_path, vid3_path)"]},{"cell_type":"markdown","metadata":{"id":"yGLtnYPAqrH_"},"source":["## Algorithm 4.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-wxoZQiqrH_"},"outputs":[],"source":["## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n","\n","def extract_arr(input_video_path, landmark_list = list(range(33))):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # we sort the landmark_list inplace for convenience\n","            landmark_list.sort()\n","            \n","            # initialize to store the key-points coordinates in the frame\n","            frame_coordinates = np.zeros((len(landmark_list),3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(len(landmark_list)):\n","                idx = landmark_list[i]\n","                x, y, z = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(len(landmark_list)):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates), landmark_list\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, len(landmark_list), 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr):\n","    \n","    # normalize the coordinates\n","    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n","    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n","    \n","    d = distance(vid1_arr, vid2_arr)\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n","    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n","    vid2_arr, _ = extract_arr(vid2_path, landmark_list)\n","\n","    score = compare_vid(vid1_arr, vid2_arr)\n","\n","    print(\"Final Score:\", score)"]},{"cell_type":"markdown","metadata":{"id":"DhQ3IP7aqrH_"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJfOlQQHqrH_"},"outputs":[],"source":["# we consider landmarks from 0 till 24: to consider only the upper body\n","landmark_list = list(range(25))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMUBYWpXqrH_","outputId":"efe68635-5690-49fd-febf-aad38e5b9867"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 100.0\n"]}],"source":["combined_compare(vid1_path, vid1_path,landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v33fWUwQqrH_","outputId":"bef83152-82bc-4768-b196-522a8369a280"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 77.4598144210633\n"]}],"source":["combined_compare(vid1_path, vid2_path,landmark_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PByPN9yPqrIA","outputId":"7c607623-c61c-49ee-d654-57e0fb9cd22d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 61.681784088694194\n"]}],"source":["combined_compare(vid1_path, vid3_path,landmark_list)"]},{"cell_type":"markdown","metadata":{"id":"U2nxQj6iqrIA"},"source":["# 4. Normalized Algorithms with added Sensitivity\n","\n","We need sensitivity hyperparameters to control how strict or lenient we would want our model to be. In our implementation:\n","- Higher Sensitivity => Lenient Model => Higher Average Score\n","- Lower Sensitivity => Strict Model => Lower Average Score\n","- Default Sensitivity = 1\n","\n","Adding the sensitivity hyperparameter:\n","- We keep everything the same as seen in Cube Normalization.\n","- We only tweak the compare_vid function (this takes in the arrays of coordinates extracted from the video and uses DTW to output a score) to add an extra parameter called sensitivity (with default value = 1), which is divided in the numerator along with the norm of the array of coordinates from the video extracted using the extract_arr function.\n","- This enables us to control the strictness of the model."]},{"cell_type":"markdown","metadata":{"id":"GW84fVtLqrIA"},"source":["## Algorithm 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFUFsvLgqrIA"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","## reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW while incorporating the sensitivity parameter\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","        vid1_arr[i] = vid1_arr[i]/(np.linalg.norm(vid1_arr[i]) * senstivity)\n","        vid2_arr[i] = vid2_arr[i]/(np.linalg.norm(vid2_arr[i]) * senstivity)\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new, senstivity)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"cYw34vJGqrIA"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"markdown","metadata":{"id":"DRineKqPqrIA"},"source":["### `senstivity = 0.5`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSu6bu-QqrIB","outputId":"fdcc44d0-e85b-41c5-f9fb-cfae78d570a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [52.19760186 42.06616013 43.85654318 45.95627098 42.35559631 42.65965891\n"," 43.25726706 66.84132094 61.59396165 67.58222832 67.85674525 83.98774748\n"," 78.84565854 79.50496764 68.4130681  60.68745396 49.24710954 54.40691904\n"," 43.06827785 54.57750674 44.3427424  57.59982076 49.74350029 75.01235632\n"," 72.67392663 65.48866964 76.30392597 84.0481244  86.45436187 86.43598534\n"," 87.11849659 81.08129293 86.18000878]\n","Average Score: 63.68015985989418\n"]}],"source":["combined_compare(vid1_path, vid2_path, 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjVvgYqAqrIB","outputId":"04012caf-3ce5-4c12-b15c-14ea2aded382"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [41.29295678 44.08425509 44.87839439 45.74451249 41.79114755 42.26352283\n"," 42.95928539 53.2985213  47.02970258 47.56363577 44.92573436 63.08399221\n"," 59.93505596 32.54323265 50.63552474 18.36559147 30.96139195  5.70526184\n"," 19.41361143  4.7532758  15.7526076  11.08567471 22.1567631  48.02546074\n"," 51.3375741  47.31422008 55.16559206 58.69210896 59.39012603 60.05147107\n"," 60.06133551 57.18115343 60.44134728]\n","Average Score: 42.05709215927326\n"]}],"source":["combined_compare(vid1_path, vid3_path, 0.5)"]},{"cell_type":"markdown","metadata":{"id":"Ae8NdCzuqrIB"},"source":["### `senstivity = 1` (Default)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73VarRcvqrIB","outputId":"4d4eab10-beb3-496e-cca1-b946fe407374"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [76.09880093 71.03308007 71.92827159 72.97813549 71.17779816 71.32982945\n"," 71.62863353 83.42066047 80.79698083 83.79111416 83.92837262 91.99387374\n"," 89.42282927 89.75248382 84.20653405 80.34372698 74.62355477 77.20345952\n"," 71.53413893 77.28875337 72.1713712  78.79991038 74.87175014 87.50617816\n"," 86.33696332 82.74433482 88.15196298 92.0240622  93.22718093 93.21799267\n"," 93.55924829 90.54064646 93.09000439]\n","Average Score: 81.84007992994708\n"]}],"source":["combined_compare(vid1_path, vid2_path, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQJhE8W1qrIB","outputId":"438acd67-4c58-4bfb-ef0e-e1b06ffe9e06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [70.64647839 72.04212754 72.4391972  72.87225625 70.89557378 71.13176141\n"," 71.47964269 76.64926065 73.51485129 73.78181789 72.46286718 81.5419961\n"," 79.96752798 66.27161632 75.31776237 59.18279573 65.48069597 52.85263092\n"," 59.70680572 52.3766379  57.8763038  55.54283735 61.07838155 74.01273037\n"," 75.66878705 73.65711004 77.58279603 79.34605448 79.69506302 80.02573554\n"," 80.03066776 78.59057671 80.22067364]\n","Average Score: 71.02854607963663\n"]}],"source":["combined_compare(vid1_path, vid3_path, 1)"]},{"cell_type":"markdown","metadata":{"id":"TGQHGZx6qrIB"},"source":["### `senstivity = 1.5`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJH265n1qrIB","outputId":"269833bb-a19f-4813-8872-6d745722bf31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [84.06586729 80.68872004 81.28551439 81.98542366 80.78519877 80.88655297\n"," 81.08575569 88.94710698 87.19798722 89.19407611 89.28558175 94.66258249\n"," 92.94855285 93.16832255 89.4710227  86.89581799 83.08236985 84.80230635\n"," 81.02275928 84.85916891 81.4475808  85.86660692 83.24783343 91.67078544\n"," 90.89130888 88.49622321 92.10130866 94.68270813 95.48478729 95.47866178\n"," 95.70616553 93.69376431 95.39333626]\n","Average Score: 87.89338661996472\n"]}],"source":["combined_compare(vid1_path, vid2_path, 1.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUtcLyF1qrIB","outputId":"11b84f9a-ee2c-4695-c9c2-1e3ea7380db2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [80.43098559 81.36141836 81.62613146 81.9148375  80.59704918 80.75450761\n"," 80.98642846 84.43284043 82.34323419 82.52121192 81.64191145 87.69466407\n"," 86.64501865 77.51441088 83.54517491 72.78853049 76.98713065 68.56842061\n"," 73.13787048 68.25109193 71.91753587 70.36189157 74.05225437 82.67515358\n"," 83.77919137 82.43807336 85.05519735 86.23070299 86.46337534 86.68382369\n"," 86.68711184 85.72705114 86.81378243]\n","Average Score: 80.68569738642442\n"]}],"source":["combined_compare(vid1_path, vid3_path, 1.5)"]},{"cell_type":"markdown","metadata":{"id":"yIWvTyyxqrIC"},"source":["### `senstivity = 2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lVsn--FqrIC","outputId":"2144ab18-aa95-478e-d3b6-abf8b6ed67cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [88.04940047 85.51654003 85.9641358  86.48906775 85.58889908 85.66491473\n"," 85.81431676 91.71033024 90.39849041 91.89555708 91.96418631 95.99693687\n"," 94.71141464 94.87624191 92.10326703 90.17186349 87.31177738 88.60172976\n"," 85.76706946 88.64437668 86.0856856  89.39995519 87.43587507 93.75308908\n"," 93.16848166 91.37216741 94.07598149 96.0120311  96.61359047 96.60899634\n"," 96.77962415 95.27032323 96.54500219]\n","Average Score: 90.92003996497354\n"]}],"source":["combined_compare(vid1_path, vid2_path, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASi-Mx9aqrIC","outputId":"3ded313e-38fe-4654-9bd9-cf46e9fbcf62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [85.32323919 86.02106377 86.2195986  86.43612812 85.44778689 85.56588071\n"," 85.73982135 88.32463032 86.75742564 86.89090894 86.23143359 90.77099805\n"," 89.98376399 83.13580816 87.65888119 79.59139787 82.74034799 76.42631546\n"," 79.85340286 76.18831895 78.9381519  77.77141868 80.53919078 87.00636519\n"," 87.83439353 86.82855502 88.79139801 89.67302724 89.84753151 90.01286777\n"," 90.01533388 89.29528836 90.11033682]\n","Average Score: 85.51427303981832\n"]}],"source":["combined_compare(vid1_path, vid3_path, 2)"]},{"cell_type":"markdown","metadata":{"id":"r12OyJtGqrIC"},"source":["### `senstivity = 5`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFodQMDgqrIC","outputId":"c1c94029-150e-458c-ca50-488f01c748a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [95.21976019 94.20661601 94.38565432 94.5956271  94.23555963 94.26596589\n"," 94.32572671 96.68413209 96.15939617 96.75822283 96.78567452 98.39877475\n"," 97.88456585 97.95049676 96.84130681 96.0687454  94.92471095 95.4406919\n"," 94.30682779 95.45775067 94.43427424 95.75998208 94.97435003 97.50123563\n"," 97.26739266 96.54886696 97.6303926  98.40481244 98.64543619 98.64359853\n"," 98.71184966 98.10812929 98.61800088]\n","Average Score: 96.36801598598942\n"]}],"source":["combined_compare(vid1_path, vid2_path, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYHN-wKnqrIC","outputId":"4708fe81-2356-469e-aa6a-e0e50603ea9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [94.12929568 94.40842551 94.48783944 94.57445125 94.17911476 94.22635228\n"," 94.29592854 95.32985213 94.70297026 94.75636358 94.49257344 96.30839922\n"," 95.9935056  93.25432326 95.06355247 91.83655915 93.09613919 90.57052618\n"," 91.94136114 90.47532758 91.57526076 91.10856747 92.21567631 94.80254607\n"," 95.13375741 94.73142201 95.51655921 95.8692109  95.9390126  96.00514711\n"," 96.00613355 95.71811534 96.04413473]\n","Average Score: 94.20570921592733\n"]}],"source":["combined_compare(vid1_path, vid3_path, 5)"]},{"cell_type":"markdown","metadata":{"id":"VLIl0zPWqrIC"},"source":["## Algorithm 6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRfasjHQqrIC"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity=1):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr * senstivity)\n","    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr * senstivity)\n","    \n","    d = distance(vid1_arr, vid2_arr)\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n","\n","    print(\"Final Score:\", score)"]},{"cell_type":"markdown","metadata":{"id":"OJoyj-DQqrID"},"source":["We now use the videos in **Dataset 0** to test our algorithms."]},{"cell_type":"markdown","metadata":{"id":"PKIvR0IJqrID"},"source":["### `senstivity = 0.5`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpoww0YpqrID","outputId":"080f8d64-408c-4171-8f08-38dac392a350"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 61.31977003065177\n"]}],"source":["combined_compare(vid1_path, vid2_path, 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZa1GxjKqrID","outputId":"84f6ea16-eff3-4e34-934c-e55f8a96ea74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 34.65284698794811\n"]}],"source":["combined_compare(vid1_path, vid3_path, 0.5)"]},{"cell_type":"markdown","metadata":{"id":"LVjFE-1aqrID"},"source":["### `senstivity = 1` (Default)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwxxkUKjqrID","outputId":"90c63c8d-0b2c-4bbc-f018-89ca48a86743"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 80.65988501532588\n"]}],"source":["combined_compare(vid1_path, vid2_path, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8xqPcGhqrID","outputId":"cc907cb5-f5dd-42f2-9780-5079013dc766"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 67.32642349397406\n"]}],"source":["combined_compare(vid1_path, vid3_path, 1)"]},{"cell_type":"markdown","metadata":{"id":"6cLHu9QQqrID"},"source":["### `senstivity = 1.5`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiHVvYaBqrIE","outputId":"10872891-313c-4f4d-8cac-4bd6e2fdd1bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 87.10659001021726\n"]}],"source":["combined_compare(vid1_path, vid2_path, 1.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCfif66uqrIE","outputId":"5ca546d3-fb1c-4277-e76d-cc70e0320d87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 78.2176156626494\n"]}],"source":["combined_compare(vid1_path, vid3_path, 1.5)"]},{"cell_type":"markdown","metadata":{"id":"QckIRa3_qrIE"},"source":["### `senstivity = 2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBD3MFy2qrIE","outputId":"555af92b-8c2b-4439-837f-0b7b511150a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 90.32994250766293\n"]}],"source":["combined_compare(vid1_path, vid2_path, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwFA82DxqrIE","outputId":"e88c4a5d-a695-4068-c398-f501a31e43dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 83.66321174698703\n"]}],"source":["combined_compare(vid1_path, vid3_path, 2)"]},{"cell_type":"markdown","metadata":{"id":"nX8ehJDwqrIE"},"source":["### `senstivity = 5`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZtvI8A4qrIE","outputId":"c31b61df-0818-4556-8bdd-298bc93082ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 96.13197700306517\n"]}],"source":["combined_compare(vid1_path, vid2_path, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKMIf2WuqrIF","outputId":"12a38d4c-088c-4207-9539-1c3f77369742"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 93.46528469879479\n"]}],"source":["combined_compare(vid1_path, vid3_path, 5)"]},{"cell_type":"markdown","metadata":{"id":"ZZqfiT04qrIF"},"source":["# 5. Comparison without incorporating any Angle Invariance\n","\n","Inorder to view the improvements obtained by incorporating angle invariance into the previous algorithms, we test our most recent algorithm on **Dataset 1**, which contains the video of the subject doing bicep curls, captured from 3 angles, i.e. front, left and right.\n","\n","We test these videos by comparing them using **Algorithm 5** and **Algorithm 6**"]},{"cell_type":"markdown","metadata":{"id":"dMWQL99aqrIF"},"source":["## Algorithm 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4-Am7IBqrIF"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","## reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW while incorporating the sensitivity parameter\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","        vid1_arr[i] = vid1_arr[i]/(np.linalg.norm(vid1_arr[i]) * senstivity)\n","        vid2_arr[i] = vid2_arr[i]/(np.linalg.norm(vid2_arr[i]) * senstivity)\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new, senstivity)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTuXNLl9qrIF","outputId":"5d27aa13-afc4-4143-d608-d8dd6b4056c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [45.22674208 53.4325719  55.64925515 57.89520312 50.84884936 51.94202676\n"," 53.12908517 53.73710754 64.71518806 59.35555499 57.38926076 47.94843278\n"," 36.2996597  52.91552326 45.6973902  21.96842416 45.20449684 -4.83100935\n"," 35.16916514 -6.77046046 24.04786828  6.19902415 41.13754584 59.35762923\n"," 59.44756476 76.35263031 64.58019943 66.28150802 57.38213105 61.41047399\n"," 57.57897927 55.99940455 66.06404367]\n","Average Score: 47.659438476444215\n"]}],"source":["combined_compare(vidf_path, vidl_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZbqNlEEqrIG","outputId":"948e4ffa-65e1-443d-e9ec-4363a00df416"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [54.67339965 64.00474909 65.9591996  67.86586825 55.4702046  56.61552616\n"," 58.10813624 52.64060764 56.93837906 68.65334328 58.59660477 -3.44145224\n"," 31.97151369 18.97767591 38.18179489 25.37866009 28.47155748 -5.28306657\n","  9.69910369 -5.90577003  9.32165017  8.39902529 17.21961296 41.40006457\n"," 57.03371723 50.10330405 59.77757616 32.08080955 42.4314739  18.71239601\n"," 29.30638285 50.32866455 51.74349256]\n","Average Score: 38.34649106405652\n"]}],"source":["combined_compare(vidl_path, vidr_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XD00SB04qrIG","outputId":"9bef0313-c435-4056-d844-78a132f0614c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Scores List:\n"," [ 65.84589319  67.10406967  67.55816017  67.69236121  64.24567159\n","  63.23967121  62.21962484  70.15788802  36.1333119   67.9651035\n","  63.59649441  40.12820616   0.62910235  55.73664817  13.25159761\n","  82.12647498   2.33949145  79.81775631 -13.47594076  77.75256505\n"," -11.7287956   82.17856165  -5.12174903  67.90564144  54.08030043\n","  66.0554927   71.55136753  57.37749607  70.61079732  50.78935092\n","  64.70348539  66.67736493  52.73294585]\n","Average Score: 52.178073049184114\n"]}],"source":["combined_compare(vidr_path, vidf_path)"]},{"cell_type":"markdown","metadata":{"id":"aFxDw5zFqrIG"},"source":["## Algorithm 6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUykDPEeqrIG"},"outputs":[],"source":["## to create a helper function to extract (n, 33, 3) dimensional array to store the normalized coordinates (via cube normalization) of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity=1):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr * senstivity)\n","    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr * senstivity)\n","    \n","    d = distance(vid1_arr, vid2_arr)\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n","\n","    print(\"Final Score:\", score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jytTYy8qqrIG","outputId":"bc3588b7-bfb9-46f5-8a5f-d8c604b40f6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 43.1257580930832\n"]}],"source":["combined_compare(vidf_path, vidl_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ADNBosFqrIH","outputId":"8cdf9f1e-c6f3-42ee-c084-fa76fa2491df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 22.883248620537046\n"]}],"source":["combined_compare(vidl_path, vidr_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Kp0RQROqrIH","outputId":"9d61ee9c-c9ad-400a-91bb-6a0ca9ff202d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Final Score: 43.54172035829884\n"]}],"source":["combined_compare(vidr_path, vidf_path)"]},{"cell_type":"markdown","metadata":{"id":"yf018D1_qrIH"},"source":["# 6. Implementing Angle Invariance: Euler's Rotation Matrix\n","\n","- We rotate the coordinates using Euler's rotation matrix about `y-axis` for various angles and collect the coordinates for each frame.\n","- If we fix an angle $\\theta$ to rotate, we obtain $\\frac{2 \\pi}{\\theta}$ no. of coordinates for each key-point in each frame.\n","- In essence we obtain $\\frac{2 \\pi}{\\theta}$ many videos to compare against the trainer video and we output the best score."]},{"cell_type":"markdown","metadata":{"id":"vyFbKUsEqrIH"},"source":["## Algorithm 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-lroY_XqrIH"},"outputs":[],"source":["## This is the Euler's Rotation Matrix (Ry) to rotate the 3D coordinates in anti-clockwise direction w.r.t. the +ve y axis by an angle of theta\n","def Ry(theta):\n","  return np.matrix([[ np.cos(theta), 0,  np.sin(theta)],\n","                    [ 0            , 1,  0            ],\n","                    [-np.sin(theta), 0,  np.cos(theta)]])\n","\n","\n","## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n","\n","def extract_arr(input_video_path):\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)\n","\n","\n","## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame after rotation (modified version of extract_arr method)\n","\n","def extract_arr_rot(input_video_path, rot_angle=None):\n","    # if no rotation return the original coordinates array\n","    if rot_angle == None or rot_angle == 0:\n","        return extract_arr(input_video_path)\n","\n","    # extract the rotation matrix\n","    rot_mat_y = Ry(rot_angle)\n","\n","    coordinates = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks (key-points and their coordinates)\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","\n","            # initialize frame_coordinates to store the coordinates of the 33 key-points per frame\n","            frame_coordinates = np.zeros((33,3))\n","            \n","            # inialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                # rotate the coordinates by multiplying with rotation matrix\n","                C = rot_mat_y@np.array((x,y,z))\n","                x, y, z = C[0,0], C[0,1], C[0,2]\n","\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z   \n","            \n","            # normalize frame_coordinates\n","            for i in range(33):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","            \n","            coordinates.append(frame_coordinates)\n","\n","        # Releasing the video capture device\n","        cap.release()\n","\n","        return np.array(coordinates)    \n","\n","\n","# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n","\n","def my_reshape(input_arr):\n","    n_frames = input_arr.shape[0]\n","    n_landmarks = input_arr.shape[1]\n","\n","    # new array of desired shape\n","    new_arr = np.zeros((n_landmarks, n_frames, 3))\n","\n","    for f in range(n_frames):\n","        for i in range(n_landmarks):\n","            new_arr[i][f] = input_arr[f][i]\n","    \n","    return new_arr\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (33, n, 3) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n","    n_landmarks = vid1_arr.shape[0]\n","    \n","    scores = np.zeros(n_landmarks)\n","    \n","    for i in range (n_landmarks):\n","        # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","        vid1_arr[i] = vid1_arr[i]/(np.linalg.norm(vid1_arr[i]) * senstivity)\n","        vid2_arr[i] = vid2_arr[i]/(np.linalg.norm(vid2_arr[i]) * senstivity)\n","        \n","        # calculate distance\n","        d = distance(vid1_arr[i], vid2_arr[i])\n","        # we give a score out of 100\n","        d_score = 100 - (d*100)\n","        scores[i] = d_score\n","    \n","    return scores\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_arr(vid1_path)\n","    vid2_arr  = extract_arr(vid2_path)\n","\n","    vid1_arr_new = my_reshape(vid1_arr)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","\n","    scores = compare_vid(vid1_arr_new, vid2_arr_new, senstivity)\n","\n","    print(\"Scores List:\\n\", scores)\n","    print(\"Average Score:\", scores.mean())\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","## we add an extra step of rotating the coordinates and collecting them for further comparison to compare two (33, n, 3) dimensional vectors\n","\n","def combined_compare_rot(vid1_path, vid2_path, senstivity=1, rot_angle=None):\n","    # list of scores\n","    scores = {}\n","    \n","    vid2_arr  = extract_arr(vid2_path)\n","    vid2_arr_new = my_reshape(vid2_arr)\n","    \n","    # no rotation cases\n","    if rot_angle==None or rot_angle == 0:\n","        return combined_compare(vid1_path, vid2_path, senstivity)\n","    \n","    n_rotations = int(2*np.pi//rot_angle)\n","\n","    for i in range(n_rotations):\n","        theta = rot_angle*i\n","        \n","        vid1_arr = extract_arr_rot(vid1_path, theta)\n","        vid1_arr_new = my_reshape(vid1_arr)\n","\n","        score = compare_vid(vid1_arr_new, vid2_arr_new, senstivity).mean()\n","        scores[theta*(180/np.pi)] = score\n","\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"RRJV_sbTqrIH"},"source":["We now use the videos in **Dataset 1** to test our algorithms for angle invariance."]},{"cell_type":"markdown","metadata":{"id":"nx03IPiaqrII"},"source":["### $\\theta = \\frac{\\pi}{2}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCXAFqr9qrII","outputId":"ab285474-098c-42f3-a248-4421999d3d9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 47.659438476444215,\n"," 90.0: 54.02428022596708,\n"," 180.0: 38.287762524885515,\n"," 270.0: 38.14814235803677}"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/2\n","combined_compare_rot(vidf_path, vidl_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScP4LKcWqrII","outputId":"c85acc25-11ca-4817-c3b1-66c229b61c40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 38.34649106405652,\n"," 90.0: 18.98437412497976,\n"," 180.0: 48.24839240653634,\n"," 270.0: 60.44612304276004}"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/2\n","combined_compare_rot(vidl_path, vidr_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYZykHe8qrII","outputId":"e8ab882d-543f-474c-cc23-8e3d5fc703bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 52.178073049184114,\n"," 90.0: 51.090001666712304,\n"," 180.0: 33.45393718279474,\n"," 270.0: 43.88127734300271}"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/2\n","combined_compare_rot(vidr_path, vidf_path, rot_angle=theta)"]},{"cell_type":"markdown","metadata":{"id":"nMQ2Qf4MqrII"},"source":["### $\\theta = \\frac{\\pi}{4}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D0TscimqrII","outputId":"946286f1-8a24-4878-bdab-99d893e6faec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 47.659438476444215,\n"," 45.0: 60.30430232656137,\n"," 90.0: 54.02428022596708,\n"," 135.0: 48.154522230399344,\n"," 180.0: 38.287762524885515,\n"," 225.0: 38.72428958495459,\n"," 270.0: 38.14814235803677,\n"," 315.0: 44.545602472111796}"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/4\n","combined_compare_rot(vidf_path, vidl_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubLsiLQJqrII","outputId":"aaeb51c2-1767-4775-e6d0-a9c285b9b5a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 38.34649106405652,\n"," 45.0: 29.597350251645953,\n"," 90.0: 18.98437412497976,\n"," 135.0: 22.821740366390834,\n"," 180.0: 48.24839240653635,\n"," 225.0: 52.09853614133803,\n"," 270.0: 60.446123042760036,\n"," 315.0: 55.29132903344268}"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/4\n","combined_compare_rot(vidl_path, vidr_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_hccXFxqrII","outputId":"fb91e53a-afee-4175-caf5-9d7ff84a872b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 52.178073049184114,\n"," 45.0: 60.35806076708979,\n"," 90.0: 51.09000166671231,\n"," 135.0: 29.181638962246964,\n"," 180.0: 33.45393718279474,\n"," 225.0: 42.74894720281564,\n"," 270.0: 43.88127734300271,\n"," 315.0: 54.329924607042685}"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/4\n","combined_compare_rot(vidr_path, vidf_path, rot_angle=theta)"]},{"cell_type":"markdown","metadata":{"id":"ZiBvmytOqrII"},"source":["### $\\theta = \\frac{\\pi}{6}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoaTqafMqrII","outputId":"d4e89c83-d1c8-4ffd-fe72-0d32971f724a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 47.659438476444215,\n"," 29.999999999999996: 58.781760367498286,\n"," 59.99999999999999: 58.89815144623445,\n"," 90.0: 54.02428022596708,\n"," 119.99999999999999: 50.55557022595661,\n"," 149.99999999999997: 46.30758400398886,\n"," 180.0: 38.287762524885515,\n"," 210.0: 37.93348158634412,\n"," 239.99999999999997: 39.210504535344306,\n"," 270.0: 38.14814235803677,\n"," 299.99999999999994: 43.28729659360412,\n"," 330.0: 44.78307060550464}"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/6\n","combined_compare_rot(vidf_path, vidl_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NWUfwzmqrII","outputId":"67ab561c-f060-4aa8-8185-de10783ee70b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 38.34649106405652,\n"," 29.999999999999996: 30.73449418164807,\n"," 59.99999999999999: 27.963597941140304,\n"," 90.0: 18.984374124979755,\n"," 119.99999999999999: 21.5046834745022,\n"," 149.99999999999997: 24.85387949798147,\n"," 180.0: 48.24839240653635,\n"," 210.0: 52.05661520949112,\n"," 239.99999999999997: 53.00924977222898,\n"," 270.0: 60.446123042760036,\n"," 299.99999999999994: 55.62914894785719,\n"," 330.0: 54.75260432404643}"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/6\n","combined_compare_rot(vidl_path, vidr_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qt4hfAmUqrIJ","outputId":"2c449130-7046-488d-b919-107e1298d94a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 52.178073049184114,\n"," 29.999999999999996: 60.93226729045125,\n"," 59.99999999999999: 59.67668193616592,\n"," 90.0: 51.090001666712304,\n"," 119.99999999999999: 30.773560387242483,\n"," 149.99999999999997: 28.53088981539551,\n"," 180.0: 33.45393718279474,\n"," 210.0: 42.25463960514128,\n"," 239.99999999999997: 43.064568327309466,\n"," 270.0: 43.88127734300271,\n"," 299.99999999999994: 53.92688582665791,\n"," 330.0: 54.29148697602684}"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/6\n","combined_compare_rot(vidr_path, vidf_path, rot_angle=theta)"]},{"cell_type":"markdown","metadata":{"id":"EK5kGSagqrIJ"},"source":["### $\\theta = \\frac{\\pi}{8}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vE-PhhcXqrIJ","outputId":"93840551-300e-4ddf-a739-7c5a98237e05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 47.659438476444215,\n"," 22.5: 57.228361420429806,\n"," 45.0: 60.30430232656137,\n"," 67.5: 57.21640102568671,\n"," 90.0: 54.02428022596708,\n"," 112.5: 51.86413581712696,\n"," 135.0: 48.154522230399344,\n"," 157.5: 45.32285240240143,\n"," 180.0: 38.287762524885515,\n"," 202.5: 37.239242324854196,\n"," 225.0: 38.72428958495459,\n"," 247.49999999999997: 39.38510957826169,\n"," 270.0: 38.14814235803677,\n"," 292.5: 42.295004582297224,\n"," 315.0: 44.5456024721118,\n"," 337.5: 44.76715560641582}"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/8\n","combined_compare_rot(vidf_path, vidl_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_zXsGveqrIJ","outputId":"c3698d1d-3e7a-441b-ddd6-9cbf081e5700"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 38.34649106405652,\n"," 22.5: 31.939510389559018,\n"," 45.0: 29.597350251645953,\n"," 67.5: 26.790305115310787,\n"," 90.0: 18.984374124979755,\n"," 112.5: 21.05255730422534,\n"," 135.0: 22.821740366390834,\n"," 157.5: 26.634373282613527,\n"," 180.0: 48.24839240653635,\n"," 202.5: 52.230318037641304,\n"," 225.0: 52.09853614133803,\n"," 247.49999999999997: 54.06973832915243,\n"," 270.0: 60.44612304276004,\n"," 292.5: 56.04153855987537,\n"," 315.0: 55.29132903344268,\n"," 337.5: 54.22376565435354}"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/8\n","combined_compare_rot(vidl_path, vidr_path, rot_angle=theta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AIdPIzpqrIJ","outputId":"b707fd45-1bc0-44db-f6f6-f220de2e679e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Ignoring empty camera frame.\n"]},{"data":{"text/plain":["{0.0: 52.178073049184114,\n"," 22.5: 61.30688816454947,\n"," 45.0: 60.35806076708979,\n"," 67.5: 58.83640115867573,\n"," 90.0: 51.090001666712304,\n"," 112.5: 32.196241038427026,\n"," 135.0: 29.181638962246964,\n"," 157.5: 28.888419020272995,\n"," 180.0: 33.45393718279474,\n"," 202.5: 41.71112620127972,\n"," 225.0: 42.74894720281565,\n"," 247.49999999999997: 43.42666721675701,\n"," 270.0: 43.88127734300271,\n"," 292.5: 53.3728479891264,\n"," 315.0: 54.329924607042685,\n"," 337.5: 54.36619212113806}"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["theta = np.pi/8\n","combined_compare_rot(vidr_path, vidf_path, rot_angle=theta)"]},{"cell_type":"markdown","metadata":{"id":"Hoi3m7WgqrIJ"},"source":["# 7. Implementing Angle Invariance: Comparing Joint Angles"]},{"cell_type":"markdown","metadata":{"id":"zefpJBWRqrIJ"},"source":["## Algorithm 8.1 (without Cube Normalization)\n","\n","- First, we extract the coordinates of 33 keypoints using Mediapipe Blazepose. These are the keypoints and their corresponding labels:\n","\n","![](https://mediapipe.dev/images/mobile/pose_tracking_full_body_landmarks.png)\n","\n","- We add 4 more key-points namely:\n","    - `33`: left-hand\n","    - `34`: neck\n","    - `35`: right-hand\n","    - `36`: middle-pelvis\n","\n","- We then consider 18-relevant joint angles, using the key-point coordinates:\n","    - ![](https://i.postimg.cc/zBSdvndp/image.png)\n","\n","- We extract these 18 angles for each frame, and compile these into a `(n, 18)` shaped array, where `n` is the no. of frames in the video.\n","\n","- Then we use DTW to compare these arrays of the videos to compare and obtain a score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJz3x5P7qrIJ"},"outputs":[],"source":["## calculating the joint angles\n","def calculate_angle(a, b, c):\n","    # we will be calculating angle ABC\n","    \n","    # extract unit vector BA:\n","    v1 = a - b\n","    v1_u = v1/np.linalg.norm(v1)\n","    # extract unit vector BC:\n","    v2 = c - b\n","    v2_u = v2/np.linalg.norm(v2)\n","\n","    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n","\n","## these are 18 tuples of keypoints to extract 18 relevant angles\n","\n","keypoint_index = np.array([\n","    [33, 16, 14],\n","    [16, 14, 12],\n","    [14, 12, 11],\n","    [14, 12, 24],\n","    [ 0, 34, 12],\n","    [ 0, 34, 36],\n","    [12, 11, 13],\n","    [13, 11, 23],\n","    [11, 13, 15],\n","    [13, 15, 35],\n","    [12, 24, 26],\n","    [26, 24, 23],\n","    [24, 23, 25],\n","    [11, 23, 25],\n","    [24, 26, 28],\n","    [23, 25, 27],\n","    [26, 28, 32],\n","    [25, 27, 31]\n","])\n","\n","## to create a helper function to extract (n, 18) dimensional array to store the coordinates of each landmark in each frame\n","\n","def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n","    angles = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","                # print(landmarks)\n","            except:\n","                pass\n","            \n","            # initialize frame_coordinates to store the coordinates of the keypoints per frame\n","            # we initialize to size (37,3) instead of (33,3) because we include 4 more key-points, for angle calculation\n","            frame_coordinates = np.zeros((37,3))\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","            \n","            # adding 4 new keypoints to the already existing 33 keypoints\n","\n","            # left hand key-point\n","            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n","            # right hand key-point\n","            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n","            # neck key-point\n","            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n","            # middle pelvis key-point\n","            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n","\n","            # initialize array to hold angles per frame\n","            angle = np.zeros(18)\n","\n","            # extract angles based on the relevant key-points from the keypoint_index\n","            for i in range(18):\n","                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n","            \n","            angles.append(angle)\n","\n","    # Releasing the video capture\n","    cap.release()\n","\n","    return np.array(angles)\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, 18) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n","    \n","    \n","    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","\n","    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n","    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n","    \n","    # calculate distance\n","    d = distance(vid1_arr, vid2_arr)\n","    # we give a score out of 100\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_angle_arr(vid1_path)\n","    vid2_arr  = extract_angle_arr(vid2_path)\n","\n","    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n","\n","    print(\"Score:\", score)\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"EqmeDzlSqrIK"},"source":["We now use the videos in **Dataset 1** to test our algorithms for angle invariance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Txna_o2hqrIK","outputId":"cace068c-cd85-4ad3-afd5-eaeb28fe34ce"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"]},{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Score: 62.77854002852074\n"]},{"data":{"text/plain":["62.77854002852074"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["combined_compare(vidf_path, vidl_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCM_E-gCqrIK","outputId":"90bc4340-1159-4d8f-8e3c-57de25082317"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Score: 48.29008589082052\n"]},{"data":{"text/plain":["48.29008589082052"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["combined_compare(vidl_path, vidr_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGen3pUTqrIK","outputId":"1f436df1-2a00-42d7-a8cd-78c8e98cee9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Score: 60.443947858380085\n"]},{"data":{"text/plain":["60.443947858380085"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["combined_compare(vidr_path, vidf_path)"]},{"cell_type":"markdown","metadata":{"id":"-45m4kn-qrIK"},"source":["## Algorithm 8.2 (with Cube Normalization)\n","\n","- First, we extract the coordinates of 33 keypoints using Mediapipe Blazepose. These are the keypoints and their corresponding labels:\n","\n","![](https://mediapipe.dev/images/mobile/pose_tracking_full_body_landmarks.png)\n","\n","- We add 4 more key-points namely:\n","    - `33`: left-hand\n","    - `34`: neck\n","    - `35`: right-hand\n","    - `36`: middle-pelvis\n","\n","- We employ cube-normalization by applying the following transformation to each coordinate in each frame:\n","    - $(x,y,z) \\mapsto (\\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}, \\frac{y - y_{\\text{min}}}{y_{\\text{max}} - y_{\\text{min}}}, \\frac{z - z_{\\text{min}}}{z_{\\text{max}} - z_{\\text{min}}})$\n","    - This makes $(x_{\\text{min}}, y_{\\text{min}}, z_{\\text{min}})$ the origin of the unit cube coordinate system.\n","\n","- We then consider 18-relevant joint angles, using the key-point coordinates:\n","    - ![](https://i.postimg.cc/zBSdvndp/image.png)\n","\n","- We extract these 18 angles for each frame, and compile these into a `(n, 18)` shaped array, where `n` is the no. of frames in the video.\n","\n","- Then we use DTW to compare these arrays of the videos to compare and obtain a score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fau2jHJ3qrIK"},"outputs":[],"source":["## calculating the joint angles\n","def calculate_angle(a, b, c):\n","    # we will be calculating angle ABC\n","    \n","    # extract unit vector BA:\n","    v1 = a - b\n","    v1_u = v1/np.linalg.norm(v1)\n","    # extract unit vector BC:\n","    v2 = c - b\n","    v2_u = v2/np.linalg.norm(v2)\n","\n","    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n","\n","\n","## these are 18 tuples of keypoints to extract 18 relevant angles\n","\n","keypoint_index = np.array([\n","    [33, 16, 14],\n","    [16, 14, 12],\n","    [14, 12, 11],\n","    [14, 12, 24],\n","    [ 0, 34, 12],\n","    [ 0, 34, 36],\n","    [12, 11, 13],\n","    [13, 11, 23],\n","    [11, 13, 15],\n","    [13, 15, 35],\n","    [12, 24, 26],\n","    [26, 24, 23],\n","    [24, 23, 25],\n","    [11, 23, 25],\n","    [24, 26, 28],\n","    [23, 25, 27],\n","    [26, 28, 32],\n","    [25, 27, 31]\n","])\n","\n","## to create a helper function to extract (n, 18) dimensional array to store the coordinates of each landmark in each frame\n","\n","def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n","    angles = []\n","    \n","    cap = cv2.VideoCapture(input_video_path)\n","\n","    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n","        while cap.isOpened():\n","            # Reading frames from video\n","            success, frame = cap.read()\n","            \n","            if not success:\n","                print(\"Ignoring empty camera frame.\")\n","                break\n","            \n","            # Recolor image to RGB\n","            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            image.flags.writeable = False\n","        \n","            # Make detection\n","            results = pose.process(image)\n","        \n","            # Recolor back to BGR\n","            image.flags.writeable = True\n","            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","            \n","            # Extract landmarks\n","            try:\n","                landmarks = results.pose_landmarks.landmark\n","            except:\n","                pass\n","            \n","            # initialize frame_coordinates to store the coordinates of the keypoints per frame\n","            # we initialize to size (37,3) instead of (33,3) because we include 4 more key-points, for angle calculation\n","            frame_coordinates = np.zeros((37,3))\n","\n","            # initialize min-max coordinates variables\n","            x_min, y_min, z_min = np.inf, np.inf, np.inf\n","            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n","\n","            # extract (x,y,z) coordinate of each landmark\n","            for i in range(33):\n","                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n","                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n","\n","                # checking and updating the min coordinates\n","                if x < x_min:\n","                    x_min = x\n","                if y < y_min:\n","                    y_min = y\n","                if z < z_min:\n","                    z_min = z\n","                \n","                # checking and updating the max coordinates\n","                if x > x_max:\n","                    x_max = x\n","                if y > y_max:\n","                    y_max = y\n","                if z > z_max:\n","                    z_max = z\n","            \n","            # adding 4 new keypoints to the already existing 33 keypoints\n","\n","            # left hand key-point\n","            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n","            # right hand key-point\n","            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n","            # neck key-point\n","            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n","            # middle pelvis key-point\n","            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n","\n","            # normalize frame_coordinates\n","            for i in range(37):\n","                # normalizing x coordinate\n","                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n","                # normalizing y coordinate\n","                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n","                # normalizing z coordinate\n","                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n","\n","            # initialize array to hold angles per frame\n","            angle = np.zeros(18)\n","\n","            # extract angles based on the relevant key-points from the keypoint_index\n","            for i in range(18):\n","                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n","            \n","            angles.append(angle)\n","\n","    # Releasing the video capture\n","    cap.release()\n","\n","    return np.array(angles)\n","\n","\n","from dtaidistance.dtw_ndim import distance, distance_fast\n","\n","## to compare 2 videos of dim (n, 18) using DTW\n","\n","def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n","    \n","    \n","    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n","\n","    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n","    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n","    \n","    # calculate distance\n","    d = distance(vid1_arr, vid2_arr)\n","    # we give a score out of 100\n","    d_score = 100 - (d*100)\n","    \n","    return d_score\n","\n","\n","## helper function for comparing videos by path: just including the pre-processing steps in the previous function\n","\n","def combined_compare(vid1_path, vid2_path, senstivity=1):\n","    vid1_arr = extract_angle_arr(vid1_path)\n","    vid2_arr  = extract_angle_arr(vid2_path)\n","\n","    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n","\n","    print(\"Score:\", score)\n","    return score"]},{"cell_type":"markdown","metadata":{"id":"JTQRIcZ0qrIK"},"source":["We now use the videos in **Dataset 1** to test our algorithms for angle invariance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxoCN5xQqrIL","outputId":"60fd6aab-ba8b-4411-b8ea-948ca202aabb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Score: 76.83928496785356\n"]},{"data":{"text/plain":["76.83928496785356"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["combined_compare(vidf_path, vidl_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oa8y_FKRqrIL","outputId":"ea56c111-3062-48ef-d0a5-2b163d55abdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Score: 74.22925426625005\n"]},{"data":{"text/plain":["74.22925426625005"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["combined_compare(vidl_path, vidr_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlDGTw1HqrIL","outputId":"02035e46-6898-414e-a4df-1f2ddea68c82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignoring empty camera frame.\n","Ignoring empty camera frame.\n","Score: 75.6500236414682\n"]},{"data":{"text/plain":["75.6500236414682"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["combined_compare(vidr_path, vidf_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Yz16t-hqrIL"},"outputs":[],"source":["%%time"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('cvenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"e4ed39f92146e9c2db1a29729bb43dc2eb9581f5c121f265c48b6aafb302e3e7"}},"colab":{"provenance":[],"collapsed_sections":["fbYkweoyqrHz","Gwo_xPO1qrH1","dJwxyEsUqrH3","d_5ETqImqrH6","HdRf1nDvqrH8","JzV9yH-kqrH9","yGLtnYPAqrH_","GW84fVtLqrIA","DRineKqPqrIA","Ae8NdCzuqrIB","TGQHGZx6qrIB","yIWvTyyxqrIC","r12OyJtGqrIC","VLIl0zPWqrIC","PKIvR0IJqrID","LVjFE-1aqrID","6cLHu9QQqrID","QckIRa3_qrIE","nX8ehJDwqrIE","dMWQL99aqrIF","aFxDw5zFqrIG","vyFbKUsEqrIH","nx03IPiaqrII","nMQ2Qf4MqrII","ZiBvmytOqrII","EK5kGSagqrIJ","zefpJBWRqrIJ"]}},"nbformat":4,"nbformat_minor":0}