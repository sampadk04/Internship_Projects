{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[43728]: Class CaptureDelegate is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x121bde538) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x122e30860). One of the two will be used. Which one is undefined.\n",
      "objc[43728]: Class CVWindow is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x121bde588) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11ea3ca68). One of the two will be used. Which one is undefined.\n",
      "objc[43728]: Class CVView is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x121bde5b0) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11ea3ca90). One of the two will be used. Which one is undefined.\n",
      "objc[43728]: Class CVSlider is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x121bde5d8) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11ea3cab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapose utils\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_checker(video_path):\n",
    "    # Establishing connection with the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Properties\n",
    "    # extracting video properties\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    size = (width, height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('a','v','c','1')\n",
    "    op_path = os.path.join('data', 'output_videos', 'output.avi')\n",
    "    # Video Writer \n",
    "    video_writer = cv2.VideoWriter(op_path, fourcc, fps, size)\n",
    "\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "            \n",
    "            video_writer.write(image)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "        # Releasing video writer\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "video_checker(vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Checker with white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/white_bg.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting background path\n",
    "bg_path = os.path.join('data', 'white_bg.jpg')\n",
    "bg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_path = os.path.join('data', 'white_bg.jpg')\n",
    "bg_main = cv2.imread(bg_path)\n",
    "bg_height, bg_width = bg_main.shape[0], bg_main.shape[1]\n",
    "\n",
    "def video_checker_bg(video_path):\n",
    "    # Establishing connection with the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Properties\n",
    "    # extracting video properties\n",
    "    height = bg_height\n",
    "    width = bg_width\n",
    "    size = (width, height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('a','v','c','1')\n",
    "    op_path = os.path.join('data', 'output_videos', 'output.avi')\n",
    "    # Video Writer\n",
    "    video_writer = cv2.VideoWriter(op_path, fourcc, fps, size)\n",
    "\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # read the white background\n",
    "            bg = cv2.imread(bg_path)\n",
    "            \n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(bg, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "            \n",
    "            video_writer.write(bg)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "        # Releasing video writer\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "video_checker_bg(vid1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the Box of rectangle per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_checker2(video_path):\n",
    "    # Establishing connection with the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Properties\n",
    "    # extracting video properties\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    size = (width, height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('a','v','c','1')\n",
    "    op_path = os.path.join('data', 'output_videos', 'output.avi')\n",
    "    # Video Writer \n",
    "    video_writer = cv2.VideoWriter(op_path, fourcc, fps, size)\n",
    "\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "\n",
    "\n",
    "            # Draw a rectangle in x-y plane, enclosing all the keypoints\n",
    "            top_left = (x_min, y_max)\n",
    "            bottom_right = (x_max, y_min)\n",
    "\n",
    "            top_left = tuple(np.multiply(top_left, size).astype(int))\n",
    "            bottom_right = tuple(np.multiply(bottom_right, size).astype(int))\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right,(0,255,0),3)\n",
    "            \n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "            \n",
    "            video_writer.write(image)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "        # Releasing video writer\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n"
     ]
    }
   ],
   "source": [
    "video_checker2(vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the Box of rectangle on bg after rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this causes anti-clockwise rotation w.r.t. the +ve y axis by an angle of theta\n",
    "def Ry(theta):\n",
    "  return np.matrix([[ np.cos(theta), 0,  np.sin(theta)],\n",
    "                    [ 0            , 1,  0            ],\n",
    "                    [-np.sin(theta), 0,  np.cos(theta)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix([\n",
    "    [ 0, 0, 1],\n",
    "    [ 0, 1, 0],\n",
    "    [-1, 0, 0]\n",
    "])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.matrix([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0],\n",
       "        [ 1],\n",
       "        [-1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1,1,0) should go to (0,1,-1) after a rotation on 90 degrees anti-clockwise\n",
    "X@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_path = os.path.join('data', 'white_bg.jpg')\n",
    "bg_main = cv2.imread(bg_path)\n",
    "bg_height, bg_width = bg_main.shape[0], bg_main.shape[1]\n",
    "\n",
    "def video_checker_rot_bg(video_path, rot_angle = np.pi/4):\n",
    "    # Establishing connection with the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Properties\n",
    "    # extracting video properties\n",
    "    height = bg_height\n",
    "    width = bg_width\n",
    "    size = (width, height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('a','v','c','1')\n",
    "    op_path = os.path.join('data', 'output_videos', 'output.avi')\n",
    "    # Video Writer\n",
    "    video_writer = cv2.VideoWriter(op_path, fourcc, fps, size)\n",
    "\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "            \n",
    "            # rotation matrix\n",
    "            rot_mat_y = Ry(rot_angle)\n",
    "            \n",
    "            print(frame_coordinates.shape)\n",
    "\n",
    "            # updated frame coordinates after rotating this w.r.t. y-axis\n",
    "            frame_coordinates = frame_coordinates @ np.transpose(rot_mat_y)\n",
    "\n",
    "            print(frame_coordinates.shape)\n",
    "\n",
    "            # update (x,y,z) coordinates for each landmark in results\n",
    "            for i in range(33):\n",
    "                #landmarks[i].x, landmarks[i].y, landmarks[i].z = frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2]\n",
    "                landmarks[i].x = frame_coordinates[i][0]\n",
    "        \n",
    "            # read the white background\n",
    "            bg = cv2.imread(bg_path)\n",
    "            \n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(bg, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "            \n",
    "            video_writer.write(bg)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "        # Releasing video writer\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_checker2(video_path):\n",
    "    # Establishing connection with the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Properties\n",
    "    # extracting video properties\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    size = (width, height)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc('a','v','c','1')\n",
    "    op_path = os.path.join('data', 'output_videos', 'output.avi')\n",
    "    # Video Writer \n",
    "    video_writer = cv2.VideoWriter(op_path, fourcc, fps, size)\n",
    "\n",
    "    ## Setup mediapipe instance\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # initializing overall x,z, min-max coordinates for finding axis of rotation\n",
    "            X_min, Z_min = np.inf, np.inf\n",
    "            X_max, Z_max = -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "            \n",
    "            # checking and updating overall min coordinates\n",
    "            if x_min < X_min:\n",
    "                X_min = x_min\n",
    "            if z_min < Z_min:\n",
    "                Z_min = z_min\n",
    "            \n",
    "            # checking and updating overall max coordinates\n",
    "            if x_max > X_max:\n",
    "                X_max = x_max\n",
    "            if z_max > Z_max:\n",
    "                Z_max = z_max\n",
    "\n",
    "\n",
    "            # Draw a rectangle in x-y plane, enclosing all the keypoints\n",
    "            top_left = (x_min, y_max)\n",
    "            bottom_right = (x_max, y_min)\n",
    "\n",
    "            top_left = tuple(np.multiply(top_left, size).astype(int))\n",
    "            bottom_right = tuple(np.multiply(bottom_right, size).astype(int))\n",
    "\n",
    "            cv2.rectangle(image, top_left, bottom_right,(0,255,0),3)\n",
    "            \n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                    )\n",
    "            \n",
    "            video_writer.write(image)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "        # Releasing video writer\n",
    "        video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample_videos/curls3-1.mp4'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid1_path = os.path.join('data', 'sample_videos', 'curls3-1.mp4')\n",
    "vid1_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample_videos/curls3-2.mp4'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid2_path = os.path.join('data', 'sample_videos', 'curls3-2.mp4')\n",
    "vid2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sample_videos/curls3-3.mp4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid3_path = os.path.join('data', 'sample_videos', 'curls3-3.mp4')\n",
    "vid3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 1.1 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates\n",
    "        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n",
    "        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n",
    "\n",
    "    print(\"Scores List:\\n\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 21.4 s, sys: 403 ms, total: 21.8 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [77.64975877 77.51754387 77.55602626 77.60267612 77.69823648 77.72383293\n",
      " 77.76272665 77.87427589 78.52889323 77.89025921 78.09730396 79.23676164\n",
      " 79.29086474 84.07772858 80.88352592 76.7209857  69.06288637 75.14808023\n",
      " 67.02120509 74.80446291 66.30122747 74.34038988 67.36503401 80.99545498\n",
      " 81.17073534 79.05177224 77.54065719 77.68160555 76.73332758 77.80852284\n",
      " 76.83242531 76.98610586 76.16014382]\n",
      "Average Score: 76.7004677761122\n",
      "CPU times: user 16.8 s, sys: 319 ms, total: 17.1 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [90.36712509 90.48350076 90.4214465  90.35077107 89.98707817 89.96469313\n",
      " 89.93794297 90.28094662 88.2643645  90.54121127 89.79774844 91.68475828\n",
      " 87.10831677 92.99999853 87.92232255 88.48792062 81.77806401 88.23772274\n",
      " 80.51475224 88.83917342 81.23785002 87.93834118 81.52105547 92.34317024\n",
      " 91.37831678 91.96892653 92.49469029 92.16970594 92.91024929 92.23795573\n",
      " 92.89406741 92.15368687 92.04274271]\n",
      "Average Score: 89.4321398829224\n",
      "CPU times: user 20.1 s, sys: 354 ms, total: 20.5 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 1.2 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the coordinates of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n",
    "\n",
    "def extract_arr(input_video_path, landmark_list = list(range(33))):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # we sort the landmark_list inplace for convenience\n",
    "            landmark_list.sort()\n",
    "\n",
    "            frame_coordinates = np.zeros((len(landmark_list),3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(len(landmark_list)):\n",
    "                idx = landmark_list[i]\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates), landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates\n",
    "        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n",
    "        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n",
    "    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n",
    "    vid2_arr, _  = extract_arr(vid2_path, landmark_list)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n",
    "\n",
    "    print(\"Scores List:\\n\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider landmarks from 0 till 24\n",
    "landmark_list = list(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid1_path, landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [77.64975877 77.51754387 77.55602626 77.60267612 77.69823648 77.72383293\n",
      " 77.76272665 77.87427589 78.52889323 77.89025921 78.09730396 79.23676164\n",
      " 79.29086474 84.07772858 80.88352592 76.7209857  69.06288637 75.14808023\n",
      " 67.02120509 74.80446291 66.30122747 74.34038988 67.36503401 80.99545498\n",
      " 81.17073534]\n",
      "Average Score: 76.49283504900352\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid2_path, landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [90.36712509 90.48350076 90.4214465  90.35077107 89.98707817 89.96469313\n",
      " 89.93794297 90.28094662 88.2643645  90.54121127 89.79774844 91.68475828\n",
      " 87.10831677 92.99999853 87.92232255 88.48792062 81.77806401 88.23772274\n",
      " 80.51475224 88.83917342 81.23785002 87.93834118 81.52105547 92.34317024\n",
      " 91.37831678]\n",
      "Average Score: 88.4955436546632\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid3_path, landmark_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 2.1 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    # normalize the coordinates\n",
    "    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n",
    "    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n",
    "    \n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr)\n",
    "\n",
    "    print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 100.0\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 72.13579475938042\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 85.61251023753023\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 2.2 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the coordinates of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n",
    "\n",
    "def extract_arr(input_video_path, landmark_list = list(range(33))):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # we sort the landmark_list inplace for convenience\n",
    "            landmark_list.sort()\n",
    "\n",
    "            frame_coordinates = np.zeros((len(landmark_list),3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(len(landmark_list)):\n",
    "                idx = landmark_list[i]\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates), landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    \n",
    "    # normalize the coordinates\n",
    "    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n",
    "    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n",
    "    \n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n",
    "    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n",
    "    vid2_arr, _ = extract_arr(vid2_path, landmark_list)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr)\n",
    "\n",
    "    print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider landmarks from 0 till 24\n",
    "landmark_list = list(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 100.0\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid1_path,landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 70.6449503249911\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid2_path, landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 83.77952859907592\n"
     ]
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid3_path, landmark_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 3.1 TEST: Normalizing the subject\n",
    "\n",
    "## Idea: Capturing Subject in a cuboid\n",
    "- Capture min-max for x, y and z coordinates to extract a cuboid\n",
    "- Normalize into a unit lengthed cube\n",
    "- Translation of  min coordinate to 0\n",
    "- Repeat the above steps in each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## min_coordinates, max_coordinates = (n, 3), (n, 3)\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "            \n",
    "            # normalize frame_coordinates\n",
    "            for i in range(33):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates\n",
    "        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n",
    "        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n",
    "\n",
    "    print(\"Scores List:\\n\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 34.8 s, sys: 643 ms, total: 35.5 s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [70.64647839 72.04212754 72.4391972  72.87225625 70.89557378 71.13176141\n",
      " 71.47964269 76.64926065 73.51485129 73.78181789 72.46286718 81.5419961\n",
      " 79.96752798 66.27161632 75.31776237 59.18279573 65.48069597 52.85263092\n",
      " 59.70680572 52.3766379  57.8763038  55.54283735 61.07838155 74.01273037\n",
      " 75.66878705 73.65711004 77.58279603 79.34605448 79.69506302 80.02573554\n",
      " 80.03066776 78.59057671 80.22067364]\n",
      "Average Score: 71.02854607963663\n",
      "CPU times: user 27.2 s, sys: 514 ms, total: 27.7 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [76.09880093 71.03308007 71.92827159 72.97813549 71.17779816 71.32982945\n",
      " 71.62863353 83.42066047 80.79698083 83.79111416 83.92837262 91.99387374\n",
      " 89.42282927 89.75248382 84.20653405 80.34372698 74.62355477 77.20345952\n",
      " 71.53413893 77.28875337 72.1713712  78.79991038 74.87175014 87.50617816\n",
      " 86.33696332 82.74433482 88.15196298 92.0240622  93.22718093 93.21799267\n",
      " 93.55924829 90.54064646 93.09000439]\n",
      "Average Score: 81.84007992994708\n",
      "CPU times: user 32.7 s, sys: 622 ms, total: 33.3 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 3.2 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, len(landmark_list), 3) dimensional array to store the coordinates of each landmark in the landmark_list in each frame (we only consider the landmarks in this list `landmark_list`)\n",
    "\n",
    "def extract_arr(input_video_path, landmark_list = list(range(33))):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # we sort the landmark_list inplace for convenience\n",
    "            landmark_list.sort()\n",
    "            \n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((len(landmark_list),3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(len(landmark_list)):\n",
    "                idx = landmark_list[i]\n",
    "                x, y, z = landmarks[idx].x, landmarks[idx].y, landmarks[idx].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z   \n",
    "            \n",
    "            # normalize frame_coordinates\n",
    "            for i in range(len(landmark_list)):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates), landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, len(landmark_list), 3) dimensional array to a (len(landmark_list), n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates\n",
    "        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n",
    "        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, landmark_list = list(range(33))):\n",
    "    vid1_arr, _ = extract_arr(vid1_path, landmark_list)\n",
    "    vid2_arr, _  = extract_arr(vid2_path, landmark_list)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n",
    "\n",
    "    print(\"Scores List:\\n\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider landmarks from 0 till 24\n",
    "landmark_list = list(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 19.4 s, sys: 462 ms, total: 19.9 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path, landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [77.35746198 77.4755386  77.96513494 78.49942307 77.01953939 77.18586414\n",
      " 77.48336462 81.13451253 79.70358058 79.9590351  79.50166629 81.43148999\n",
      " 85.53170833 65.59291866 76.88160367 52.32549177 69.21050326 49.27063941\n",
      " 66.29886869 49.27494289 64.55502344 50.97623    66.85073321 75.39640356\n",
      " 76.66159941]\n",
      "Average Score: 71.7417311011076\n",
      "CPU times: user 15.2 s, sys: 332 ms, total: 15.6 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path, landmark_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [79.29076533 74.62334593 75.25919102 76.04498242 76.75394097 76.75762224\n",
      " 76.875806   85.18942254 85.8666015  85.01640652 86.24877752 93.15801846\n",
      " 88.64307079 87.08987375 83.57118547 88.67744092 74.7101584  87.44175462\n",
      " 73.97601561 87.14164671 75.5954874  87.54069156 76.52657353 84.99043884\n",
      " 85.98794896]\n",
      "Average Score: 82.11908668037474\n",
      "CPU times: user 18 s, sys: 343 ms, total: 18.4 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path, landmark_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 4.1 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "\n",
    "            # normalize frame_coordinates\n",
    "            for i in range(33):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    # normalize the coordinates\n",
    "    vid1_arr = vid1_arr/np.linalg.norm(vid1_arr)\n",
    "    vid2_arr = vid2_arr/np.linalg.norm(vid2_arr)\n",
    "    \n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr)\n",
    "\n",
    "    print(\"Final Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 100.0\n",
      "CPU times: user 12 s, sys: 336 ms, total: 12.3 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 67.32642349397406\n",
      "CPU times: user 10.2 s, sys: 365 ms, total: 10.6 s\n",
      "Wall time: 9.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Final Score: 80.65988501532588\n",
      "CPU times: user 11.4 s, sys: 357 ms, total: 11.8 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 5.1: Including Sensitivity on top of Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## min_coordinates, max_coordinates = (n, 3), (n, 3)\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z   \n",
    "            \n",
    "            # normalize frame_coordinates\n",
    "            for i in range(33):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "        vid1_arr[i] = vid1_arr[i]/(np.linalg.norm(vid1_arr[i]) * senstivity)\n",
    "        vid2_arr[i] = vid2_arr[i]/(np.linalg.norm(vid2_arr[i]) * senstivity)\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new, senstivity)\n",
    "\n",
    "    print(\"Scores List:\\n\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sensitivity = 0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 35 s, sys: 708 ms, total: 35.8 s\n",
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [41.29295678 44.08425509 44.87839439 45.74451249 41.79114755 42.26352283\n",
      " 42.95928539 53.2985213  47.02970258 47.56363577 44.92573436 63.08399221\n",
      " 59.93505596 32.54323265 50.63552474 18.36559147 30.96139195  5.70526184\n",
      " 19.41361143  4.7532758  15.7526076  11.08567471 22.1567631  48.02546074\n",
      " 51.3375741  47.31422008 55.16559206 58.69210896 59.39012603 60.05147107\n",
      " 60.06133551 57.18115343 60.44134728]\n",
      "Average Score: 42.05709215927326\n",
      "CPU times: user 27 s, sys: 522 ms, total: 27.5 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [52.19760186 42.06616013 43.85654318 45.95627098 42.35559631 42.65965891\n",
      " 43.25726706 66.84132094 61.59396165 67.58222832 67.85674525 83.98774748\n",
      " 78.84565854 79.50496764 68.4130681  60.68745396 49.24710954 54.40691904\n",
      " 43.06827785 54.57750674 44.3427424  57.59982076 49.74350029 75.01235632\n",
      " 72.67392663 65.48866964 76.30392597 84.0481244  86.45436187 86.43598534\n",
      " 87.11849659 81.08129293 86.18000878]\n",
      "Average Score: 63.68015985989418\n",
      "CPU times: user 32.3 s, sys: 609 ms, total: 32.9 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sensitivity = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 21.8 s, sys: 444 ms, total: 22.3 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [97.06464784 97.20421275 97.24391972 97.28722562 97.08955738 97.11317614\n",
      " 97.14796427 97.66492606 97.35148513 97.37818179 97.24628672 98.15419961\n",
      " 97.9967528  96.62716163 97.53177624 95.91827957 96.5480696  95.28526309\n",
      " 95.97068057 95.23766379 95.78763038 95.55428374 96.10783816 97.40127304\n",
      " 97.56687871 97.365711   97.7582796  97.93460545 97.9695063  98.00257355\n",
      " 98.00306678 97.85905767 98.02206736]\n",
      "Average Score: 97.10285460796366\n",
      "CPU times: user 17 s, sys: 319 ms, total: 17.3 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [97.60988009 97.10330801 97.19282716 97.29781355 97.11777982 97.13298295\n",
      " 97.16286335 98.34206605 98.07969808 98.37911142 98.39283726 99.19938737\n",
      " 98.94228293 98.97524838 98.42065341 98.0343727  97.46235548 97.72034595\n",
      " 97.15341389 97.72887534 97.21713712 97.87999104 97.48717501 98.75061782\n",
      " 98.63369633 98.27443348 98.8151963  99.20240622 99.32271809 99.32179927\n",
      " 99.35592483 99.05406465 99.30900044]\n",
      "Average Score: 98.18400799299471\n",
      "CPU times: user 20.4 s, sys: 373 ms, total: 20.7 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems `sensitivity=10` is too lenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sensitivity = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 21.8 s, sys: 464 ms, total: 22.3 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [94.12929568 94.40842551 94.48783944 94.57445125 94.17911476 94.22635228\n",
      " 94.29592854 95.32985213 94.70297026 94.75636358 94.49257344 96.30839922\n",
      " 95.9935056  93.25432326 95.06355247 91.83655915 93.09613919 90.57052618\n",
      " 91.94136114 90.47532758 91.57526076 91.10856747 92.21567631 94.80254607\n",
      " 95.13375741 94.73142201 95.51655921 95.8692109  95.9390126  96.00514711\n",
      " 96.00613355 95.71811534 96.04413473]\n",
      "Average Score: 94.20570921592733\n",
      "CPU times: user 17.2 s, sys: 372 ms, total: 17.5 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [95.21976019 94.20661601 94.38565432 94.5956271  94.23555963 94.26596589\n",
      " 94.32572671 96.68413209 96.15939617 96.75822283 96.78567452 98.39877475\n",
      " 97.88456585 97.95049676 96.84130681 96.0687454  94.92471095 95.4406919\n",
      " 94.30682779 95.45775067 94.43427424 95.75998208 94.97435003 97.50123563\n",
      " 97.26739266 96.54886696 97.6303926  98.40481244 98.64543619 98.64359853\n",
      " 98.71184966 98.10812929 98.61800088]\n",
      "Average Score: 96.36801598598942\n",
      "CPU times: user 20.4 s, sys: 374 ms, total: 20.8 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sensitivity = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 22.2 s, sys: 472 ms, total: 22.7 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [90.2154928  90.68070918 90.81306573 90.95741875 90.29852459 90.3772538\n",
      " 90.49321423 92.21642022 91.1716171  91.26060596 90.82095573 93.84733203\n",
      " 93.32250933 88.75720544 91.77258746 86.39426524 88.49356532 84.28421031\n",
      " 86.56893524 84.12554597 85.95876793 85.18094578 87.02612718 91.33757679\n",
      " 91.88959568 91.21903668 92.52759868 93.11535149 93.23168767 93.34191185\n",
      " 93.34355592 92.86352557 93.40689121]\n",
      "Average Score: 90.34284869321222\n",
      "CPU times: user 17.1 s, sys: 350 ms, total: 17.4 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [92.03293364 90.34436002 90.6427572  90.99271183 90.39259939 90.44327648\n",
      " 90.54287784 94.47355349 93.59899361 94.59703805 94.64279087 97.33129125\n",
      " 96.47427642 96.58416127 94.73551135 93.44790899 91.54118492 92.40115317\n",
      " 90.51137964 92.42958446 90.7237904  92.93330346 91.62391671 95.83539272\n",
      " 95.44565444 94.24811161 96.05065433 97.34135407 97.74239364 97.73933089\n",
      " 97.85308276 96.84688215 97.69666813]\n",
      "Average Score: 93.94669330998236\n",
      "CPU times: user 20.1 s, sys: 398 ms, total: 20.5 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sensitivity = 1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100.]\n",
      "Average Score: 100.0\n",
      "CPU times: user 21.7 s, sys: 440 ms, total: 22.1 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid1_path, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [80.43098559 81.36141836 81.62613146 81.9148375  80.59704918 80.75450761\n",
      " 80.98642846 84.43284043 82.34323419 82.52121192 81.64191145 87.69466407\n",
      " 86.64501865 77.51441088 83.54517491 72.78853049 76.98713065 68.56842061\n",
      " 73.13787048 68.25109193 71.91753587 70.36189157 74.05225437 82.67515358\n",
      " 83.77919137 82.43807336 85.05519735 86.23070299 86.46337534 86.68382369\n",
      " 86.68711184 85.72705114 86.81378243]\n",
      "Average Score: 80.68569738642442\n",
      "CPU times: user 17.1 s, sys: 358 ms, total: 17.5 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid2_path, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Scores List:\n",
      " [84.06586729 80.68872004 81.28551439 81.98542366 80.78519877 80.88655297\n",
      " 81.08575569 88.94710698 87.19798722 89.19407611 89.28558175 94.66258249\n",
      " 92.94855285 93.16832255 89.4710227  86.89581799 83.08236985 84.80230635\n",
      " 81.02275928 84.85916891 81.4475808  85.86660692 83.24783343 91.67078544\n",
      " 90.89130888 88.49622321 92.10130866 94.68270813 95.48478729 95.47866178\n",
      " 95.70616553 93.69376431 95.39333626]\n",
      "Average Score: 87.89338661996472\n",
      "CPU times: user 20.3 s, sys: 323 ms, total: 20.6 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(vid1_path, vid3_path, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like `sensitivity = 1.5` seems like a sweet spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Paths for newer testing (squats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right videos\n",
    "r1_path = os.path.join('data', 'sample_videos', 'R1.mp4')\n",
    "r2_path = os.path.join('data', 'sample_videos', 'R2.mp4')\n",
    "r3_path = os.path.join('data', 'sample_videos', 'R3.mp4')\n",
    "r4_path = os.path.join('data', 'sample_videos', 'R4.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong videos\n",
    "w1_path = os.path.join('data', 'sample_videos', 'W1.mp4')\n",
    "w2_path = os.path.join('data', 'sample_videos', 'W2.mp4')\n",
    "w3_path = os.path.join('data', 'sample_videos', 'W3.mp4')\n",
    "w4_path = os.path.join('data', 'sample_videos', 'W4.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing using VERSION 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_path = [r1_path, r2_path, r3_path, r4_path]\n",
    "w_path = [w1_path, w2_path, w3_path, w4_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Average Right Video Comparison Score: 80.13880997915103\n"
     ]
    }
   ],
   "source": [
    "r_score = 0\n",
    "r_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(i, len(r_path)):\n",
    "        r_score += combined_compare(r_path[i], r_path[j])\n",
    "        r_count += 1\n",
    "\n",
    "print(\"Average Right Video Comparison Score:\", r_score/r_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Average Wrong Video Comparison Score: 66.92426199589143\n"
     ]
    }
   ],
   "source": [
    "w_score = 0\n",
    "w_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(len(w_path)):\n",
    "        w_score += combined_compare(r_path[i], w_path[j])\n",
    "        w_count += 1\n",
    "\n",
    "print(\"Average Wrong Video Comparison Score:\", w_score/w_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 6.1 TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try comparing the current video with multiple trainer videos and consider the best score out of the lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## min_coordinates, max_coordinates = (n, 3), (n, 3)\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z   \n",
    "            \n",
    "            # normalize frame_coordinates\n",
    "            for i in range(33):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates\n",
    "        vid1_arr[i] = vid1_arr[i]/np.linalg.norm(vid1_arr[i])\n",
    "        vid2_arr[i] = vid2_arr[i]/np.linalg.norm(vid2_arr[i])\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid_path):\n",
    "    # input videos\n",
    "    vid_arr = extract_arr(vid_path)\n",
    "    vid_arr_new = my_reshape(vid_arr)\n",
    "\n",
    "    # trainer videos\n",
    "    r1_arr = extract_arr(r1_path)\n",
    "    r1_arr_new = my_reshape(r1_arr)\n",
    "    r3_arr = extract_arr(r3_path)\n",
    "    r3_arr_new = my_reshape(r3_arr)\n",
    "    r4_arr = extract_arr(r4_path)\n",
    "    r4_arr_new = my_reshape(r4_arr)\n",
    "\n",
    "    score = max(compare_vid(vid_arr_new, r1_arr_new),\n",
    "                compare_vid(vid_arr_new, r3_arr_new),\n",
    "                compare_vid(vid_arr_new, r4_arr_new))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "CPU times: user 1min 6s, sys: 1.51 s, total: 1min 7s\n",
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72.40808010519264"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(r2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "CPU times: user 53 s, sys: 1.38 s, total: 54.3 s\n",
      "Wall time: 51.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.9834707326342"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(w1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "CPU times: user 44.5 s, sys: 1.22 s, total: 45.7 s\n",
      "Wall time: 43.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.12152267900882"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(w2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "CPU times: user 52.6 s, sys: 1.29 s, total: 53.9 s\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.41409671364036"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined_compare(w3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 7.1 TEST: COMPARING JOINT ANGLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify and consider various relevant angles during the exercises and compare these with multiple trainer videos to obtain the best score out of the lot. This should also help us achieve angle invariance in each individual frames.\n",
    "\n",
    "- These are the angles we will be considering:\n",
    "    - ![](https://i.postimg.cc/zBSdvndp/image.png)\n",
    "\n",
    "- These are the blazepose keypoints:\n",
    "    - ![](https://i.postimg.cc/G3VDY0vT/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,1,0])\n",
    "b = np.array([0,0,0])\n",
    "c = np.array([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = calculate_angle(a,b,c)\n",
    "theta * 180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "## to create a helper function to extract (n, 18) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## these are 16 tuples of keypoints to extract 18 relevant angles\n",
    "\n",
    "keypoint_index = np.array([\n",
    "    [33, 16, 14],\n",
    "    [16, 14, 12],\n",
    "    [14, 12, 11],\n",
    "    [14, 12, 24],\n",
    "    [ 0, 34, 12],\n",
    "    [ 0, 34, 36],\n",
    "    [12, 11, 13],\n",
    "    [13, 11, 23],\n",
    "    [11, 13, 15],\n",
    "    [13, 15, 35],\n",
    "    [12, 24, 26],\n",
    "    [26, 24, 23],\n",
    "    [24, 23, 25],\n",
    "    [11, 23, 25],\n",
    "    [24, 26, 28],\n",
    "    [23, 25, 27],\n",
    "    [26, 28, 32],\n",
    "    [25, 27, 31]\n",
    "])\n",
    "\n",
    "def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n",
    "    angles = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # we initialize to size (36,3) instead of (33,3) because we include 3 more key-points, for angle calculation\n",
    "            frame_coordinates = np.zeros((37,3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "            \n",
    "            # left hand key-point\n",
    "            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n",
    "            # right hand key-point\n",
    "            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n",
    "            # neck key-point\n",
    "            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n",
    "            # middle pelvis key-point\n",
    "            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n",
    "\n",
    "            # initialize array to hold angles per frame\n",
    "            angle = np.zeros(18)\n",
    "\n",
    "            # extract angles based on the relevant key-points from the keypoint_index\n",
    "            for i in range(18):\n",
    "                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n",
    "            \n",
    "            angles.append(angle)\n",
    "\n",
    "    # Releasing the video capture device\n",
    "    cap.release()\n",
    "\n",
    "    return np.array(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (n, 18) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    \n",
    "    \n",
    "    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "\n",
    "    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n",
    "    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n",
    "    \n",
    "    # calculate distance\n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    # we give a score out of 100\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_angle_arr(vid1_path)\n",
    "    vid2_arr  = extract_angle_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n",
    "\n",
    "    print(\"Score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing bicep curls from the same angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.368957684985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.368957684985"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 87.23107693505101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.23107693505101"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing bicep curls from the different angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the correct videos with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, rf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 62.77854002852074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.77854002852074"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, rl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 60.443947858380085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.443947858380085"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, rr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 48.29008589082052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.29008589082052"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rr_path, rl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the wrong videos with right videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.52242039553799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81.52242039553799"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, wf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 65.37995407069654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.37995407069654"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, wl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 60.588743967530334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.588743967530334"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, wr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing front raises from different angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# front "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing using VERSION 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right videos\n",
    "r1_path = os.path.join('data', 'sample_videos', 'R1.mp4')\n",
    "r2_path = os.path.join('data', 'sample_videos', 'R2.mp4')\n",
    "r3_path = os.path.join('data', 'sample_videos', 'R3.mp4')\n",
    "r4_path = os.path.join('data', 'sample_videos', 'R4.mp4')\n",
    "\n",
    "# wrong videos\n",
    "w1_path = os.path.join('data', 'sample_videos', 'W1.mp4')\n",
    "w2_path = os.path.join('data', 'sample_videos', 'W2.mp4')\n",
    "w3_path = os.path.join('data', 'sample_videos', 'W3.mp4')\n",
    "w4_path = os.path.join('data', 'sample_videos', 'W4.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_path = [r1_path, r2_path, r3_path, r4_path]\n",
    "w_path = [w1_path, w2_path, w3_path, w4_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 66.10707252364585\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 73.0279461649572\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.25624091760959\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 77.67547682120691\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 83.78235589193068\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.87236515525683\n",
      "Average Right Video Comparison Score: 75.45357624576785\n"
     ]
    }
   ],
   "source": [
    "r_score = 0\n",
    "r_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(i+1, len(r_path)):\n",
    "        r_score += combined_compare(r_path[i], r_path[j])\n",
    "        r_count += 1\n",
    "\n",
    "print(\"Average Right Video Comparison Score:\", r_score/r_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.9060070368812\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.98327434463353\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 71.79029053686811\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.89560640884503\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.92103581469762\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 51.92531928942424\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.97842328881359\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 64.06492347540035\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.98877571018622\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 69.83311308274973\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 87.65968970425907\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.59413516165915\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 78.9960603874682\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 61.09825004935484\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 78.88142045553198\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.42380915203564\n",
      "Average Wrong Video Comparison Score: 72.62125836867554\n"
     ]
    }
   ],
   "source": [
    "w_score = 0\n",
    "w_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(len(w_path)):\n",
    "        w_score += combined_compare(r_path[i], w_path[j])\n",
    "        w_count += 1\n",
    "\n",
    "print(\"Average Wrong Video Comparison Score:\", w_score/w_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 7.2 TEST: COMPARING JOINT ANGLES (post cube normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "## to create a helper function to extract (n, 18) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## these are 18 tuples of keypoints to extract 18 relevant angles\n",
    "\n",
    "keypoint_index = np.array([\n",
    "    [33, 16, 14],\n",
    "    [16, 14, 12],\n",
    "    [14, 12, 11],\n",
    "    [14, 12, 24],\n",
    "    [ 0, 34, 12],\n",
    "    [ 0, 34, 36],\n",
    "    [12, 11, 13],\n",
    "    [13, 11, 23],\n",
    "    [11, 13, 15],\n",
    "    [13, 15, 35],\n",
    "    [12, 24, 26],\n",
    "    [26, 24, 23],\n",
    "    [24, 23, 25],\n",
    "    [11, 23, 25],\n",
    "    [24, 26, 28],\n",
    "    [23, 25, 27],\n",
    "    [26, 28, 32],\n",
    "    [25, 27, 31]\n",
    "])\n",
    "\n",
    "def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n",
    "    angles = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # we initialize to size (36,3) instead of (33,3) because we include 3 more key-points, for angle calculation\n",
    "            frame_coordinates = np.zeros((37,3))\n",
    "\n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "            \n",
    "            # left hand key-point\n",
    "            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n",
    "            # right hand key-point\n",
    "            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n",
    "            # neck key-point\n",
    "            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n",
    "            # middle pelvis key-point\n",
    "            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n",
    "\n",
    "            # normalize frame_coordinates\n",
    "            for i in range(37):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "\n",
    "            # initialize array to hold angles per frame\n",
    "            angle = np.zeros(18)\n",
    "\n",
    "            # extract angles based on the relevant key-points from the keypoint_index\n",
    "            for i in range(18):\n",
    "                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n",
    "            \n",
    "            angles.append(angle)\n",
    "\n",
    "    # Releasing the video capture device\n",
    "    cap.release()\n",
    "\n",
    "    return np.array(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (n, 18) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    \n",
    "    \n",
    "    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "\n",
    "    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n",
    "    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n",
    "    \n",
    "    # calculate distance\n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    # we give a score out of 100\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_angle_arr(vid1_path)\n",
    "    vid2_arr  = extract_angle_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n",
    "\n",
    "    print(\"Score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing bicep curls from the same angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.03272737020221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.03272737020221"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.87120014194673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "81.87120014194673"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(vid1_path, vid3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing bicep curls from the different angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the correct videos with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, rf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.83928496785356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.83928496785356"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, rl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 75.6500236414682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.6500236414682"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, rr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.22925426625005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.22925426625005"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rr_path, rl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the wrong videos with right videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.9938427695738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.9938427695738"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, wf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.89036478961378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.89036478961378"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, wl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.63092097672896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70.63092097672896"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(rf_path, wr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Paths for angle invariance testing (front raises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_f_path = os.path.join('data', 'sample_videos', 'fr_F.mp4')\n",
    "fr_l_path = os.path.join('data', 'sample_videos', 'fr_L.mp4')\n",
    "fr_r_path = os.path.join('data', 'sample_videos', 'fr_R.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Paths for angle invariance testing (bicep curls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right videos\n",
    "rf_path = os.path.join('data', 'sample_videos', 'curls_R-F-1.mp4')\n",
    "rl_path = os.path.join('data', 'sample_videos', 'curls_R-L-1.mp4')\n",
    "rr_path = os.path.join('data', 'sample_videos', 'curls_R-R-1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong videos\n",
    "wf_path = os.path.join('data', 'sample_videos', 'curls_W-F-1.mp4')\n",
    "wl_path = os.path.join('data', 'sample_videos', 'curls_W-L-1.mp4')\n",
    "wr_path = os.path.join('data', 'sample_videos', 'curls_W-R-1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_path = [r1_path, r2_path, r3_path, r4_path]\n",
    "w_path = [w1_path, w2_path, w3_path, w4_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 67.70235853180318\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.8332059123015\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 68.63703260346023\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.27415675339242\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 73.3882433878859\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 79.88183306102289\n",
      "Average Right Video Comparison Score: 72.45280504164435\n"
     ]
    }
   ],
   "source": [
    "r_score = 0\n",
    "r_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(i+1, len(r_path)):\n",
    "        r_score += combined_compare(r_path[i], r_path[j])\n",
    "        r_count += 1\n",
    "\n",
    "print(\"Average Right Video Comparison Score:\", r_score/r_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 63.61793173771487\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 75.7764441831329\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 64.46433743038617\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.93543521732516\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 61.5680621112317\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 53.103855955116906\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 69.04939823941248\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 62.034140239169574\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 79.09963946159831\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 66.03982335066152\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.16371235116344\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 78.5478505539156\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 73.8614746105172\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 59.770713098988196\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 69.26132729745962\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.5975897692139\n",
      "Average Wrong Video Comparison Score: 68.74323347543798\n"
     ]
    }
   ],
   "source": [
    "w_score = 0\n",
    "w_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(len(w_path)):\n",
    "        w_score += combined_compare(r_path[i], w_path[j])\n",
    "        w_count += 1\n",
    "\n",
    "print(\"Average Wrong Video Comparison Score:\", w_score/w_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 8.1 TEST: ANGLE INVARIANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We rotate the coordinates using Euler's rotation matrix about `y-axis` for various angles and collect the coordinates for each frame.\n",
    "- If we fix an angle $\\theta$ to rotate, we obtain $\\frac{2 \\pi}{\\theta}$ no. of coordinates for each key-point in each frame.\n",
    "- In essence we obtain $\\frac{2 \\pi}{\\theta}$ many videos to compare against the trainer video and we output the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this causes anti-clockwise rotation w.r.t. the +ve y axis by an angle of theta\n",
    "def Ry(theta):\n",
    "  return np.matrix([[ np.cos(theta), 0,  np.sin(theta)],\n",
    "                    [ 0            , 1,  0            ],\n",
    "                    [-np.sin(theta), 0,  np.cos(theta)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to create a helper function to extract (n, 33, 3) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## min_coordinates, max_coordinates = (n, 3), (n, 3)\n",
    "\n",
    "def extract_arr(input_video_path):\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z   \n",
    "            \n",
    "            # normalize frame_coordinates\n",
    "            for i in range(33):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts normalized coordinates in array of shape (n, 33, 3)\n",
    "def extract_arr_rot(input_video_path, rot_angle=None):\n",
    "    # if no rotation return the original coordinates array\n",
    "    if rot_angle == None or rot_angle == 0:\n",
    "        return extract_arr(input_video_path)\n",
    "\n",
    "    # extract the rotation matrix\n",
    "    rot_mat_y = Ry(rot_angle)\n",
    "\n",
    "    coordinates = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # initialize to store the key-points coordinates in the frame\n",
    "            frame_coordinates = np.zeros((33,3))\n",
    "            \n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                # rotate the coordinates by multiplying with rotation matrix\n",
    "                C = rot_mat_y@np.array((x,y,z))\n",
    "                x, y, z = C[0,0], C[0,1], C[0,2]\n",
    "\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z   \n",
    "            \n",
    "            # normalize frame_coordinates\n",
    "            for i in range(33):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "            \n",
    "            coordinates.append(frame_coordinates)\n",
    "\n",
    "        # Releasing the video capture device\n",
    "        cap.release()\n",
    "\n",
    "        return np.array(coordinates)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the (n, 33, 3) dimensional array to a (33, n, 3) dimensional array for extracting scores of each key-point\n",
    "\n",
    "def my_reshape(input_arr):\n",
    "    n_frames = input_arr.shape[0]\n",
    "    n_landmarks = input_arr.shape[1]\n",
    "\n",
    "    # new array of desired shape\n",
    "    new_arr = np.zeros((n_landmarks, n_frames, 3))\n",
    "\n",
    "    for f in range(n_frames):\n",
    "        for i in range(n_landmarks):\n",
    "            new_arr[i][f] = input_arr[f][i]\n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (33, n, 3) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    n_landmarks = vid1_arr.shape[0]\n",
    "    \n",
    "    scores = np.zeros(n_landmarks)\n",
    "    \n",
    "    for i in range (n_landmarks):\n",
    "        # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "        vid1_arr[i] = vid1_arr[i]/(np.linalg.norm(vid1_arr[i]) * senstivity)\n",
    "        vid2_arr[i] = vid2_arr[i]/(np.linalg.norm(vid2_arr[i]) * senstivity)\n",
    "        \n",
    "        # calculate distance\n",
    "        d = distance(vid1_arr[i], vid2_arr[i])\n",
    "        # we give a score out of 100\n",
    "        d_score = 100 - (d*100)\n",
    "        scores[i] = d_score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_arr(vid1_path)\n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "\n",
    "    vid1_arr_new = my_reshape(vid1_arr)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "\n",
    "    scores = compare_vid(vid1_arr_new, vid2_arr_new, senstivity)\n",
    "\n",
    "    print(\"Scores List:\\n\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "# we add an extra step of rotating the coordinates and collecting them for further comparison\n",
    "# comparing (33, n, 3) dimensional vectors\n",
    "\n",
    "def combined_compare_rot(vid1_path, vid2_path, senstivity=1, rot_angle=None):\n",
    "    # list of scores\n",
    "    scores = {}\n",
    "    \n",
    "    vid2_arr  = extract_arr(vid2_path)\n",
    "    vid2_arr_new = my_reshape(vid2_arr)\n",
    "    \n",
    "    # no rotation cases\n",
    "    if rot_angle==None or rot_angle == 0:\n",
    "        return combined_compare(vid1_path, vid2_path, senstivity)\n",
    "    \n",
    "    n_rotations = int(2*np.pi//rot_angle)\n",
    "\n",
    "    for i in range(n_rotations):\n",
    "        theta = rot_angle*i\n",
    "        \n",
    "        vid1_arr = extract_arr_rot(vid1_path, theta)\n",
    "        vid1_arr_new = my_reshape(vid1_arr)\n",
    "\n",
    "        score = compare_vid(vid1_arr_new, vid2_arr_new, senstivity).mean()\n",
    "        scores[theta*(180/np.pi)] = score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\theta = \\frac{\\pi}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.pi/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 100.0,\n",
       " 90.0: 36.349799903243515,\n",
       " 180.0: 21.732012691058518,\n",
       " 270.0: 46.53851433632036}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rf_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 47.659438476444215,\n",
       " 90.0: 17.296827832320492,\n",
       " 180.0: 38.800594644063125,\n",
       " 270.0: 74.69330853422466}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rl_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 52.178073049184114,\n",
       " 90.0: 51.090001666712304,\n",
       " 180.0: 33.45393718279474,\n",
       " 270.0: 43.88127734300271}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rr_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 38.34649106405652,\n",
       " 90.0: 32.272223192732454,\n",
       " 180.0: 47.125030315666926,\n",
       " 270.0: 48.28051935160322}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rr_path, rl_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 38.34649106405652,\n",
       " 90.0: 18.98437412497976,\n",
       " 180.0: 48.24839240653634,\n",
       " 270.0: 60.44612304276004}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rl_path, rr_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\theta = \\frac{\\pi}{4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.pi/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 47.659438476444215,\n",
       " 45.0: 36.34209886770577,\n",
       " 90.0: 17.296827832320496,\n",
       " 135.0: 21.971030916972875,\n",
       " 180.0: 38.800594644063125,\n",
       " 225.0: 53.091013807917356,\n",
       " 270.0: 74.69330853422466,\n",
       " 315.0: 65.37351821767156}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rl_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 52.178073049184114,\n",
       " 45.0: 60.35806076708979,\n",
       " 90.0: 51.09000166671231,\n",
       " 135.0: 29.181638962246964,\n",
       " 180.0: 33.45393718279474,\n",
       " 225.0: 42.74894720281564,\n",
       " 270.0: 43.88127734300271,\n",
       " 315.0: 54.329924607042685}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rr_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\theta = \\frac{\\pi}{12}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.pi/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 47.659438476444215,\n",
       " 14.999999999999998: 41.50035177150157,\n",
       " 29.999999999999996: 37.595314665457074,\n",
       " 45.0: 36.34209886770577,\n",
       " 59.99999999999999: 34.09146935263043,\n",
       " 74.99999999999999: 30.07375737899245,\n",
       " 90.0: 17.296827832320496,\n",
       " 105.0: 19.939197373172462,\n",
       " 119.99999999999999: 20.915306806686615,\n",
       " 135.0: 21.971030916972875,\n",
       " 149.99999999999997: 23.465504346633413,\n",
       " 165.0: 26.70140840653056,\n",
       " 180.0: 38.800594644063125,\n",
       " 194.99999999999997: 50.36748123687142,\n",
       " 210.0: 52.07159435726688,\n",
       " 224.99999999999997: 53.09101380791735,\n",
       " 239.99999999999997: 54.55523415369474,\n",
       " 254.99999999999997: 58.05637017255959,\n",
       " 270.0: 74.69330853422466,\n",
       " 285.0: 70.48036026758383,\n",
       " 299.99999999999994: 66.89641623280136,\n",
       " 315.0: 65.37351821767156,\n",
       " 330.0: 64.58718394691407,\n",
       " 344.99999999999994: 64.33611424375478}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rl_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: 52.178073049184114,\n",
       " 14.999999999999998: 61.70805183880099,\n",
       " 29.999999999999996: 60.93226729045125,\n",
       " 45.0: 60.35806076708979,\n",
       " 59.99999999999999: 59.67668193616592,\n",
       " 74.99999999999999: 57.346590002268776,\n",
       " 90.0: 51.090001666712304,\n",
       " 105.0: 34.7830130931842,\n",
       " 119.99999999999999: 30.773560387242483,\n",
       " 135.0: 29.181638962246964,\n",
       " 149.99999999999997: 28.53088981539552,\n",
       " 165.0: 29.83297082428405,\n",
       " 180.0: 33.45393718279474,\n",
       " 194.99999999999997: 40.51481967005261,\n",
       " 210.0: 42.25463960514128,\n",
       " 224.99999999999997: 42.74894720281565,\n",
       " 239.99999999999997: 43.064568327309466,\n",
       " 254.99999999999997: 43.958867249708206,\n",
       " 270.0: 43.88127734300271,\n",
       " 285.0: 52.12381499685775,\n",
       " 299.99999999999994: 53.92688582665791,\n",
       " 315.0: 54.329924607042685,\n",
       " 330.0: 54.29148697602684,\n",
       " 344.99999999999994: 54.52661399544107}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare_rot(rr_path, rf_path, rot_angle=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ed39f92146e9c2db1a29729bb43dc2eb9581f5c121f265c48b6aafb302e3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
