{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[45398]: Class CaptureDelegate is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x11f7f6538) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x120940860). One of the two will be used. Which one is undefined.\n",
      "objc[45398]: Class CVWindow is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x11f7f6588) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11bafca68). One of the two will be used. Which one is undefined.\n",
      "objc[45398]: Class CVView is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x11f7f65b0) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11bafca90). One of the two will be used. Which one is undefined.\n",
      "objc[45398]: Class CVSlider is implemented in both /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x11f7f65d8) and /Volumes/SKK-T7/Apps/pip_envs/cvenv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x11bafcab8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapose utils\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDEO PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid1_path = os.path.join('data', 'sample_videos', 'curls3-1.mp4')\n",
    "vid2_path = os.path.join('data', 'sample_videos', 'curls3-2.mp4')\n",
    "vid3_path = os.path.join('data', 'sample_videos', 'curls3-3.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right videos\n",
    "r1_path = os.path.join('data', 'sample_videos', 'R1.mp4')\n",
    "r2_path = os.path.join('data', 'sample_videos', 'R2.mp4')\n",
    "r3_path = os.path.join('data', 'sample_videos', 'R3.mp4')\n",
    "r4_path = os.path.join('data', 'sample_videos', 'R4.mp4')\n",
    "\n",
    "# wrong videos\n",
    "w1_path = os.path.join('data', 'sample_videos', 'W1.mp4')\n",
    "w2_path = os.path.join('data', 'sample_videos', 'W2.mp4')\n",
    "w3_path = os.path.join('data', 'sample_videos', 'W3.mp4')\n",
    "w4_path = os.path.join('data', 'sample_videos', 'W4.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right videos\n",
    "rf_path = os.path.join('data', 'sample_videos', 'curls_R-F-1.mp4')\n",
    "rl_path = os.path.join('data', 'sample_videos', 'curls_R-L-1.mp4')\n",
    "rr_path = os.path.join('data', 'sample_videos', 'curls_R-R-1.mp4')\n",
    "\n",
    "# wrong videos\n",
    "wf_path = os.path.join('data', 'sample_videos', 'curls_W-F-1.mp4')\n",
    "wl_path = os.path.join('data', 'sample_videos', 'curls_W-L-1.mp4')\n",
    "wr_path = os.path.join('data', 'sample_videos', 'curls_W-R-1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontraises\n",
    "fr_f_path = os.path.join('data', 'sample_videos', 'fr_F.mp4')\n",
    "fr_l_path = os.path.join('data', 'sample_videos', 'fr_L.mp4')\n",
    "fr_r_path = os.path.join('data', 'sample_videos', 'fr_R.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "## to create a helper function to extract (n, 18) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## these are 16 tuples of keypoints to extract 18 relevant angles\n",
    "\n",
    "keypoint_index = np.array([\n",
    "    [33, 16, 14],\n",
    "    [16, 14, 12],\n",
    "    [14, 12, 11],\n",
    "    [14, 12, 24],\n",
    "    [ 0, 34, 12],\n",
    "    [ 0, 34, 36],\n",
    "    [12, 11, 13],\n",
    "    [13, 11, 23],\n",
    "    [11, 13, 15],\n",
    "    [13, 15, 35],\n",
    "    [12, 24, 26],\n",
    "    [26, 24, 23],\n",
    "    [24, 23, 25],\n",
    "    [11, 23, 25],\n",
    "    [24, 26, 28],\n",
    "    [23, 25, 27],\n",
    "    [26, 28, 32],\n",
    "    [25, 27, 31]\n",
    "])\n",
    "\n",
    "def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n",
    "    angles = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # we initialize to size (36,3) instead of (33,3) because we include 3 more key-points, for angle calculation\n",
    "            frame_coordinates = np.zeros((37,3))\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                frame_coordinates[i][0],frame_coordinates[i][1],frame_coordinates[i][2] = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "            \n",
    "            # left hand key-point\n",
    "            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n",
    "            # right hand key-point\n",
    "            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n",
    "            # neck key-point\n",
    "            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n",
    "            # middle pelvis key-point\n",
    "            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n",
    "\n",
    "            # initialize array to hold angles per frame\n",
    "            angle = np.zeros(18)\n",
    "\n",
    "            # extract angles based on the relevant key-points from the keypoint_index\n",
    "            for i in range(18):\n",
    "                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n",
    "            \n",
    "            angles.append(angle)\n",
    "\n",
    "    # Releasing the video capture device\n",
    "    cap.release()\n",
    "\n",
    "    return np.array(angles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (n, 18) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    \n",
    "    \n",
    "    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "\n",
    "    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n",
    "    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n",
    "    \n",
    "    # calculate distance\n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    # we give a score out of 100\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_angle_arr(vid1_path)\n",
    "    vid2_arr  = extract_angle_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n",
    "\n",
    "    print(\"Score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing average score for Bicep Curls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_path = [rf_path, rl_path, rr_path]\n",
    "w_path = [wf_path, wl_path, wr_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 62.77854002852074\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 60.443947858380085\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 48.29008589082052\n",
      "Average Right Video Comparison Score: 57.170857925907114\n"
     ]
    }
   ],
   "source": [
    "r_score = 0\n",
    "r_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(i+1, len(r_path)):\n",
    "        r_score += combined_compare(r_path[i], r_path[j])\n",
    "        r_count += 1\n",
    "\n",
    "print(\"Average Right Video Comparison Score:\", r_score/r_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 81.52242039553799\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 65.37995407069654\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 60.588743967530334\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 58.808760608293106\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.46934935391677\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 48.49393124457495\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 52.04897088107992\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 48.64261177789526\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 73.7519158233781\n",
      "Average Wrong Video Comparison Score: 63.52296201365588\n"
     ]
    }
   ],
   "source": [
    "w_score = 0\n",
    "w_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(len(w_path)):\n",
    "        w_score += combined_compare(r_path[i], w_path[j])\n",
    "        w_count += 1\n",
    "\n",
    "print(\"Average Wrong Video Comparison Score:\", w_score/w_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing scores for Front Raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 67.91556020519107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.91556020519107"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(fr_f_path, fr_l_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 68.26438485845838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68.26438485845838"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(fr_f_path, fr_r_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 71.37800428522779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71.37800428522779"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(fr_l_path, fr_r_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERSION 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the joint angles\n",
    "def calculate_angle(a, b, c):\n",
    "    # we will be calculating angle ABC\n",
    "    \n",
    "    # extract unit vector BA:\n",
    "    v1 = a - b\n",
    "    v1_u = v1/np.linalg.norm(v1)\n",
    "    # extract unit vector BC:\n",
    "    v2 = c - b\n",
    "    v2_u = v2/np.linalg.norm(v2)\n",
    "\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "## to create a helper function to extract (n, 18) dimensional array to store the coordinates of each landmark in each frame\n",
    "\n",
    "## these are 18 tuples of keypoints to extract 18 relevant angles\n",
    "\n",
    "keypoint_index = np.array([\n",
    "    [33, 16, 14],\n",
    "    [16, 14, 12],\n",
    "    [14, 12, 11],\n",
    "    [14, 12, 24],\n",
    "    [ 0, 34, 12],\n",
    "    [ 0, 34, 36],\n",
    "    [12, 11, 13],\n",
    "    [13, 11, 23],\n",
    "    [11, 13, 15],\n",
    "    [13, 15, 35],\n",
    "    [12, 24, 26],\n",
    "    [26, 24, 23],\n",
    "    [24, 23, 25],\n",
    "    [11, 23, 25],\n",
    "    [24, 26, 28],\n",
    "    [23, 25, 27],\n",
    "    [26, 28, 32],\n",
    "    [25, 27, 31]\n",
    "])\n",
    "\n",
    "def extract_angle_arr(input_video_path, keypoint_index = keypoint_index):\n",
    "    angles = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.3, min_tracking_confidence=0.3) as pose:\n",
    "        while cap.isOpened():\n",
    "            # Reading frames from live feed\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                break\n",
    "            \n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "        \n",
    "            # Make detection\n",
    "            results = pose.process(image)\n",
    "        \n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                # print(landmarks)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # we initialize to size (36,3) instead of (33,3) because we include 3 more key-points, for angle calculation\n",
    "            frame_coordinates = np.zeros((37,3))\n",
    "\n",
    "            # inialize min-max coordinates variables\n",
    "            x_min, y_min, z_min = np.inf, np.inf, np.inf\n",
    "            x_max, y_max, z_max = -np.inf, -np.inf, -np.inf\n",
    "\n",
    "            # extract (x,y,z) coordinate of each landmark\n",
    "            for i in range(33):\n",
    "                x, y, z = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
    "                frame_coordinates[i][0], frame_coordinates[i][1], frame_coordinates[i][2] = x, y, z\n",
    "\n",
    "                # checking and updating the min coordinates\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "                if z < z_min:\n",
    "                    z_min = z\n",
    "                \n",
    "                # checking and updating the max coordinates\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if z > z_max:\n",
    "                    z_max = z\n",
    "            \n",
    "            # left hand key-point\n",
    "            frame_coordinates[33] = (frame_coordinates[18] + frame_coordinates[20])/2\n",
    "            # right hand key-point\n",
    "            frame_coordinates[35] = (frame_coordinates[17] + frame_coordinates[19])/2\n",
    "            # neck key-point\n",
    "            frame_coordinates[34] = (frame_coordinates[11] + frame_coordinates[12])/2\n",
    "            # middle pelvis key-point\n",
    "            frame_coordinates[36] = (frame_coordinates[23] + frame_coordinates[24])/2\n",
    "\n",
    "            # normalize frame_coordinates\n",
    "            for i in range(37):\n",
    "                # normalizing x coordinate\n",
    "                frame_coordinates[i][0] = (frame_coordinates[i][0] - x_min)/(x_max - x_min)\n",
    "                # normalizing y coordinate\n",
    "                frame_coordinates[i][1] = (frame_coordinates[i][1] - y_min)/(y_max - y_min)\n",
    "                # normalizing z coordinate\n",
    "                frame_coordinates[i][2] = (frame_coordinates[i][2] - z_min)/(z_max - z_min)\n",
    "\n",
    "            # initialize array to hold angles per frame\n",
    "            angle = np.zeros(18)\n",
    "\n",
    "            # extract angles based on the relevant key-points from the keypoint_index\n",
    "            for i in range(18):\n",
    "                angle[i] = calculate_angle(frame_coordinates[keypoint_index[i][0]], frame_coordinates[keypoint_index[i][1]], frame_coordinates[keypoint_index[i][2]])\n",
    "            \n",
    "            angles.append(angle)\n",
    "\n",
    "    # Releasing the video capture device\n",
    "    cap.release()\n",
    "\n",
    "    return np.array(angles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dtaidistance.dtw_ndim import distance, distance_fast\n",
    "\n",
    "## to compare 2 videos of dim (n, 18) using DTW (using strategy 2)\n",
    "\n",
    "def compare_vid(vid1_arr, vid2_arr, senstivity = 1):\n",
    "    \n",
    "    \n",
    "    # normalize the coordinates using sensitivity; higher sensitivity => more lenient scoring, lower sensitivity => strict scoring\n",
    "\n",
    "    vid1_arr = vid1_arr/(np.linalg.norm(vid1_arr) * senstivity)\n",
    "    vid2_arr = vid2_arr/(np.linalg.norm(vid2_arr) * senstivity)\n",
    "    \n",
    "    # calculate distance\n",
    "    d = distance(vid1_arr, vid2_arr)\n",
    "    # we give a score out of 100\n",
    "    d_score = 100 - (d*100)\n",
    "    \n",
    "    return d_score\n",
    "\n",
    "\n",
    "\n",
    "# comparing videos by path: just including the pre-processing steps in the previous function\n",
    "\n",
    "def combined_compare(vid1_path, vid2_path, senstivity=1):\n",
    "    vid1_arr = extract_angle_arr(vid1_path)\n",
    "    vid2_arr  = extract_angle_arr(vid2_path)\n",
    "\n",
    "    score = compare_vid(vid1_arr, vid2_arr, senstivity)\n",
    "\n",
    "    print(\"Score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 76.83928496785356\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 75.6500236414682\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.22925426625005\n",
      "Average Right Video Comparison Score: 75.57285429185727\n"
     ]
    }
   ],
   "source": [
    "r_score = 0\n",
    "r_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(i+1, len(r_path)):\n",
    "        r_score += combined_compare(r_path[i], r_path[j])\n",
    "        r_count += 1\n",
    "\n",
    "print(\"Average Right Video Comparison Score:\", r_score/r_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 82.9938427695738\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 74.89036478961378\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.63092097672896\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 72.04742862189255\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.57485928147821\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.45363820627182\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 66.03145508569847\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 70.3104276341097\n",
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 73.72010933335761\n",
      "Average Wrong Video Comparison Score: 73.51700518874722\n"
     ]
    }
   ],
   "source": [
    "w_score = 0\n",
    "w_count = 0\n",
    "\n",
    "for i in range(len(r_path)):\n",
    "    for j in range(len(w_path)):\n",
    "        w_score += combined_compare(r_path[i], w_path[j])\n",
    "        w_count += 1\n",
    "\n",
    "print(\"Average Wrong Video Comparison Score:\", w_score/w_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing scores for Front Raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 80.0525106255779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80.0525106255779"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(fr_f_path, fr_l_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 77.7673607806419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.7673607806419"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(fr_f_path, fr_r_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring empty camera frame.\n",
      "Ignoring empty camera frame.\n",
      "Score: 83.18607247533338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.18607247533338"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_compare(fr_l_path, fr_r_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ed39f92146e9c2db1a29729bb43dc2eb9581f5c121f265c48b6aafb302e3e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
